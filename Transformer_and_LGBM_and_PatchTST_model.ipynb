{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e8245237",
      "metadata": {
        "id": "e8245237"
      },
      "source": [
        "# Transformer (3 multimodal input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06d18860",
      "metadata": {
        "id": "06d18860",
        "outputId": "3c098eee-62d6-415b-d1ef-d0b49779b433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¥ è¯»å–æ•°æ®ä¸­...\n",
            "âœ… å…± 55381 è¡Œ, 2073 åˆ—\n",
            "ğŸ§© ç‰¹å¾æ€»æ•°: 2064\n",
            "âœ… å…±ç”Ÿæˆ 53771 ä¸ªçª—å£æ ·æœ¬\n",
            "âš ï¸ è­¦å‘Šï¼šæ­£åœ¨ä½¿ç”¨éšæœºåˆ’åˆ†ï¼Œè¿™ä¼šå¯¼è‡´æ•°æ®æ³„éœ²å’Œè™šé«˜çš„éªŒè¯åˆ†æ•°ï¼\n",
            "ğŸ“Š è®­ç»ƒé›† 48393ï¼ŒéªŒè¯é›† 5378\n",
            "âœ… DataLoader æ„å»ºå®Œæˆ\n",
            "emotion: torch.Size([64, 7, 10])\n",
            "stock: torch.Size([64, 7, 6])\n",
            "text: torch.Size([64, 7, 2048])\n",
            "label: torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ===== å‚æ•° =====\n",
        "path = \"merged.parquet\"\n",
        "window_size = 7\n",
        "train_ratio = 0.8\n",
        "batch_size = 64\n",
        "\n",
        "# ===== 1ï¸âƒ£ è¯»å– parquet =====\n",
        "print(\"ğŸ“¥ è¯»å–æ•°æ®ä¸­...\")\n",
        "df = pd.read_parquet(path)\n",
        "df = df.sort_values([\"StockCode\", \"Date\"]).reset_index(drop=True)\n",
        "print(f\"âœ… å…± {len(df)} è¡Œ, {df.shape[1]} åˆ—\")\n",
        "\n",
        "# ===== 2ï¸âƒ£ ç‰¹å¾åˆ†ç±» =====\n",
        "emotion_cols = [\n",
        "    'Emotion_Index_closing', 'Emotion_Index_trading',\n",
        "    'Total_Posts_closing', 'Total_Posts_trading',\n",
        "    'Total_Click_Count_closing', 'Total_Click_Count_trading',\n",
        "    'Emotion_Momentum_3d_closing', 'Emotion_Momentum_3d_trading',\n",
        "    'Emotion_Momentum_5d_closing', 'Emotion_Momentum_5d_trading'\n",
        "]\n",
        "stock_cols = ['log_return', 'amplitude', 'rsi_14', 'macd', 'bb_width_20', 'obv']\n",
        "text_cols = [f\"dim_{i}_{t}\" for i in range(1024) for t in (\"closing\", \"trading\")]\n",
        "\n",
        "feature_cols = emotion_cols + stock_cols + text_cols\n",
        "print(f\"ğŸ§© ç‰¹å¾æ€»æ•°: {len(feature_cols)}\")\n",
        "\n",
        "# ===== 3ï¸âƒ£ æ„é€ æ‰€æœ‰çª—å£æ ·æœ¬ =====\n",
        "samples = []\n",
        "\n",
        "for code, group in df.groupby(\"StockCode\"):\n",
        "    group = group.reset_index(drop=True)\n",
        "    if len(group) <= window_size:\n",
        "        continue\n",
        "\n",
        "    emo = torch.tensor(group[emotion_cols].values, dtype=torch.float32)\n",
        "    stk = torch.tensor(group[stock_cols].values, dtype=torch.float32)\n",
        "    txt = torch.tensor(group[text_cols].values, dtype=torch.float32)\n",
        "    label = torch.tensor(group[\"Label\"].values, dtype=torch.int64)\n",
        "    date = group[\"Date\"].values\n",
        "\n",
        "    for i in range(len(group) - window_size):\n",
        "        samples.append({\n",
        "            \"StockCode\": code,\n",
        "            \"end_date\": date[i + window_size],\n",
        "            \"emotion\": emo[i:i + window_size],\n",
        "            \"stock\": stk[i:i + window_size],\n",
        "            \"text\": txt[i:i + window_size],\n",
        "            \"label\": label[i + window_size]\n",
        "        })\n",
        "\n",
        "print(f\"âœ… å…±ç”Ÿæˆ {len(samples)} ä¸ªçª—å£æ ·æœ¬\")\n",
        "\n",
        "# # ===== 4ï¸âƒ£ è½¬æ¢ä¸º DataFrameï¼ˆä»…ç”¨äºæ—¶é—´åˆ‡åˆ†ï¼‰ =====\n",
        "# dates = [s[\"end_date\"] for s in samples]\n",
        "# dates = sorted(pd.Series(dates).unique())\n",
        "# split_date = dates[int(len(dates) * train_ratio)]\n",
        "# print(f\"ğŸ“† æŒ‰æ—¶é—´åˆ’åˆ†ï¼šè®­ç»ƒé›†æˆªæ­¢åˆ° {split_date.date()}\")\n",
        "\n",
        "# # ===== 5ï¸âƒ£ æŒ‰æ—¶é—´åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† =====\n",
        "# train_samples = [s for s in samples if s[\"end_date\"] <= split_date]\n",
        "# val_samples = [s for s in samples if s[\"end_date\"] > split_date]\n",
        "# print(f\"ğŸ“Š è®­ç»ƒé›† {len(train_samples)}ï¼ŒéªŒè¯é›† {len(val_samples)}\")\n",
        "\n",
        "# ===== 4ï¸âƒ£ è­¦å‘Šï¼šä½¿ç”¨éšæœºåˆ’åˆ†ï¼ˆä»…ç”¨äºå­¦æœ¯å®éªŒï¼Œéæ­£ç¡®åšæ³•ï¼‰ =====\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"âš ï¸ è­¦å‘Šï¼šæ­£åœ¨ä½¿ç”¨éšæœºåˆ’åˆ†ï¼Œè¿™ä¼šå¯¼è‡´æ•°æ®æ³„éœ²å’Œè™šé«˜çš„éªŒè¯åˆ†æ•°ï¼\")\n",
        "train_samples, val_samples = train_test_split(samples, test_size=0.1, random_state=42)\n",
        "print(f\"ğŸ“Š è®­ç»ƒé›† {len(train_samples)}ï¼ŒéªŒè¯é›† {len(val_samples)}\")\n",
        "\n",
        "# ===== 6ï¸âƒ£ å®šä¹‰ Dataset =====\n",
        "class MultiFeatureDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        return {\n",
        "            \"emotion\": s[\"emotion\"],\n",
        "            \"stock\": s[\"stock\"],\n",
        "            \"text\": s[\"text\"],\n",
        "            \"label\": s[\"label\"]\n",
        "        }\n",
        "\n",
        "train_set = MultiFeatureDataset(train_samples)\n",
        "val_set = MultiFeatureDataset(val_samples)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"âœ… DataLoader æ„å»ºå®Œæˆ\")\n",
        "\n",
        "# ===== 7ï¸âƒ£ æ‰“å°ç»´åº¦æ£€æŸ¥ =====\n",
        "for batch in train_loader:\n",
        "    print(\"emotion:\", batch[\"emotion\"].shape)  # [B, 30, 10]\n",
        "    print(\"stock:\", batch[\"stock\"].shape)      # [B, 30, 6]\n",
        "    print(\"text:\", batch[\"text\"].shape)        # [B, 30, 2048]\n",
        "    print(\"label:\", batch[\"label\"].shape)      # [B]\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82712f15",
      "metadata": {
        "id": "82712f15"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # shape (1, max_len, dim)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "class MultiModalTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    - stock/emo/text -> ç»Ÿä¸€é™åˆ° proj_dimï¼ˆé»˜è®¤128ï¼‰ï¼Œå†èåˆåˆ° hidden_dimï¼ˆé»˜è®¤512ï¼‰\n",
        "    - åŠ å¯å­¦ä¹ ä½ç½®ç¼–ç \n",
        "    - ç”¨ TransformerEncoder åšæ—¶é—´å»ºæ¨¡\n",
        "    - ç”¨æ³¨æ„åŠ›æ± åŒ–(learnable time attention) ä»£æ›¿ç®€å• mean æ± åŒ–\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        stock_dim: int = 6,\n",
        "        emo_dim: int = 10,\n",
        "        text_dim: int = 2048,\n",
        "        proj_dim: int = 128,\n",
        "        hidden_dim: int = 256,\n",
        "        num_heads: int = 8,\n",
        "        num_layers: int = 3,\n",
        "        num_classes: int = 3,\n",
        "        max_len: int = 512,\n",
        "        dropout: float = 0.2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # === å„æ¨¡æ€ç¼–ç  ===\n",
        "        # stockï¼šç›´æ¥çº¿æ€§æ˜ å°„åˆ° proj_dim\n",
        "        self.stock_proj = nn.Sequential(\n",
        "            nn.LayerNorm(stock_dim),\n",
        "            nn.Linear(stock_dim, proj_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(proj_dim)\n",
        "        )\n",
        "\n",
        "        # emotionï¼šå…ˆåšLayerNormç¨³å®šæ•°å€¼ï¼Œå†æ˜ å°„\n",
        "        self.emo_proj = nn.Sequential(\n",
        "            nn.LayerNorm(emo_dim),\n",
        "            nn.Linear(emo_dim, proj_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(proj_dim)\n",
        "        )\n",
        "\n",
        "        # textï¼šå¼ºåŠ›é™ç»´ 2048 -> 256 -> proj_dimï¼Œå¹¶åšLayerNorm\n",
        "        self.text_reduce = nn.Sequential(\n",
        "            nn.Linear(text_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, proj_dim),\n",
        "            nn.LayerNorm(proj_dim)\n",
        "        )\n",
        "\n",
        "        # === èåˆåˆ° hidden_dimï¼ˆå¸¦è½»å¾®æ®‹å·®æ€è·¯ï¼šå…ˆconcatå†çº¿æ€§ï¼‰ ===\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(proj_dim * 3, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.LayerNorm(hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(\n",
        "            max_len=max_len,\n",
        "            dim=hidden_dim\n",
        "        )\n",
        "\n",
        "        # === Transformer ç¼–ç å™¨ ===\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            activation=\"relu\",\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True,   # æ›´ç¨³å®š\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # === æ—¶é—´æ³¨æ„åŠ›æ± åŒ–ï¼ˆä»£æ›¿ meanï¼‰===\n",
        "        self.time_attn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        # === åˆ†ç±»å¤´ ===\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, stock_seq, emo_seq, text_seq):\n",
        "        \"\"\"\n",
        "        è¾“å…¥ï¼š\n",
        "            stock_seq: (B, T, stock_dim)\n",
        "            emo_seq:   (B, T, emo_dim)\n",
        "            text_seq:  (B, T, text_dim)\n",
        "        è¾“å‡ºï¼š\n",
        "            logits:    (B, num_classes)\n",
        "        \"\"\"\n",
        "        # 1) å„æ¨¡æ€ç‰¹å¾åˆ°åŒä¸€å°ºåº¦\n",
        "        s = self.stock_proj(stock_seq)          # (B, T, proj_dim)\n",
        "        e = self.emo_proj(emo_seq)              # (B, T, proj_dim)\n",
        "        t = self.text_reduce(text_seq)          # (B, T, proj_dim)\n",
        "\n",
        "        # 2) èåˆåˆ° hidden_dim\n",
        "        x = torch.cat([s, e, t], dim=-1)        # (B, T, 3*proj_dim)\n",
        "        x = self.fuse(x)                        # (B, T, hidden_dim)\n",
        "\n",
        "        # 3) ä½ç½®ç¼–ç \n",
        "        # T = x.size(1)\n",
        "        # x = x + self.pos_embedding[:, :T, :]    # (B, T, hidden_dim)\n",
        "        x = self.pos_encoder(x) # <--- ä¿®æ”¹åœ¨è¿™é‡Œ\n",
        "\n",
        "        # 4) æ—¶é—´ä¾èµ–ç¼–ç \n",
        "        enc_out = self.encoder(x)               # (B, T, hidden_dim)\n",
        "\n",
        "        # 5) æ³¨æ„åŠ›æ± åŒ–ï¼ˆå­¦ä¼šå…³æ³¨å…³é”®æ—¶é—´æ­¥ï¼‰\n",
        "        attn = self.time_attn(enc_out)          # (B, T, 1)\n",
        "        attn = torch.softmax(attn, dim=1)       # (B, T, 1)\n",
        "        pooled = torch.sum(enc_out * attn, dim=1)  # (B, hidden_dim)\n",
        "\n",
        "        # 6) åˆ†ç±»\n",
        "        logits = self.cls_head(pooled)          # (B, num_classes)\n",
        "        return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26ca6d81",
      "metadata": {
        "id": "26ca6d81",
        "outputId": "7d9ff506-9fdc-42c7-9d28-81e500f22e89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš–ï¸ è®¡ç®—ç±»åˆ«æƒé‡...\n",
            "ç±»åˆ«è®¡æ•°: Counter({1: 34224, 2: 7317, 0: 6852})\n",
            "è®¡ç®—å‡ºçš„ç±»åˆ«æƒé‡: tensor([2.3542, 0.4713, 2.2046], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/msai/liny0145/.conda/envs/py312/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 1.1745 | Train Acc: 18.19%   Val Loss:   1.1570 | Val Acc:   15.71%\n",
            "Validation Confusion Matrix:\n",
            "[[ 435    0  364]\n",
            " [1342    1 2439]\n",
            " [ 388    0  409]]\n",
            "âœ… Best model saved (15.71%)\n",
            "Epoch 2: Train Loss: 1.1605 | Train Acc: 33.02%   Val Loss:   1.1473 | Val Acc:   46.17%\n",
            "Validation Confusion Matrix:\n",
            "[[ 443  227  129]\n",
            " [1215 1887  680]\n",
            " [ 418  226  153]]\n",
            "âœ… Best model saved (46.17%)\n",
            "Epoch 3: Train Loss: 1.1559 | Train Acc: 39.37%   Val Loss:   1.1452 | Val Acc:   44.81%\n",
            "Validation Confusion Matrix:\n",
            "[[ 503  181  115]\n",
            " [1400 1773  609]\n",
            " [ 458  205  134]]\n",
            "Epoch 4: Train Loss: 1.1524 | Train Acc: 41.86%   Val Loss:   1.1452 | Val Acc:   52.21%\n",
            "Validation Confusion Matrix:\n",
            "[[ 274  258  267]\n",
            " [ 625 2276  881]\n",
            " [ 257  282  258]]\n",
            "âœ… Best model saved (52.21%)\n",
            "Epoch 5: Train Loss: 1.1501 | Train Acc: 43.02%   Val Loss:   1.1499 | Val Acc:   36.95%\n",
            "Validation Confusion Matrix:\n",
            "[[ 503  103  193]\n",
            " [1340 1278 1164]\n",
            " [ 476  115  206]]\n",
            "Epoch 6: Train Loss: 1.1496 | Train Acc: 43.54%   Val Loss:   1.1473 | Val Acc:   52.88%\n",
            "Validation Confusion Matrix:\n",
            "[[ 310  276  213]\n",
            " [ 706 2336  740]\n",
            " [ 298  301  198]]\n",
            "âœ… Best model saved (52.88%)\n",
            "Epoch 7: Train Loss: 1.1467 | Train Acc: 44.38%   Val Loss:   1.1530 | Val Acc:   41.89%\n",
            "Validation Confusion Matrix:\n",
            "[[ 519  154  126]\n",
            " [1411 1593  778]\n",
            " [ 506  150  141]]\n",
            "Epoch 8: Train Loss: 1.1454 | Train Acc: 44.08%   Val Loss:   1.1616 | Val Acc:   56.25%\n",
            "Validation Confusion Matrix:\n",
            "[[ 351  345  103]\n",
            " [ 823 2574  385]\n",
            " [ 328  369  100]]\n",
            "âœ… Best model saved (56.25%)\n",
            "Epoch 9: Train Loss: 1.1445 | Train Acc: 45.09%   Val Loss:   1.1523 | Val Acc:   40.03%\n",
            "Validation Confusion Matrix:\n",
            "[[ 579  123   97]\n",
            " [1757 1495  530]\n",
            " [ 580  138   79]]\n",
            "Epoch 10: Train Loss: 1.1435 | Train Acc: 44.86%   Val Loss:   1.1396 | Val Acc:   46.71%\n",
            "Validation Confusion Matrix:\n",
            "[[ 499  191  109]\n",
            " [1290 1881  611]\n",
            " [ 471  194  132]]\n",
            "Epoch 11: Train Loss: 1.1425 | Train Acc: 45.78%   Val Loss:   1.1392 | Val Acc:   42.53%\n",
            "Validation Confusion Matrix:\n",
            "[[ 514  144  141]\n",
            " [1329 1618  835]\n",
            " [ 485  157  155]]\n",
            "Epoch 12: Train Loss: 1.1412 | Train Acc: 45.13%   Val Loss:   1.1434 | Val Acc:   39.51%\n",
            "Validation Confusion Matrix:\n",
            "[[ 519  134  146]\n",
            " [1433 1461  888]\n",
            " [ 523  129  145]]\n",
            "Epoch 13: Train Loss: 1.1404 | Train Acc: 46.34%   Val Loss:   1.1385 | Val Acc:   42.19%\n",
            "Validation Confusion Matrix:\n",
            "[[ 521  150  128]\n",
            " [1415 1626  741]\n",
            " [ 520  155  122]]\n",
            "Epoch 14: Train Loss: 1.1386 | Train Acc: 46.28%   Val Loss:   1.1560 | Val Acc:   54.03%\n",
            "Validation Confusion Matrix:\n",
            "[[ 375  287  137]\n",
            " [ 827 2385  570]\n",
            " [ 354  297  146]]\n",
            "Epoch 15: Train Loss: 1.1357 | Train Acc: 47.77%   Val Loss:   1.1508 | Val Acc:   36.07%\n",
            "Validation Confusion Matrix:\n",
            "[[ 614   93   92]\n",
            " [1949 1251  582]\n",
            " [ 614  108   75]]\n",
            "Epoch 16: Train Loss: 1.1352 | Train Acc: 47.27%   Val Loss:   1.1492 | Val Acc:   50.17%\n",
            "Validation Confusion Matrix:\n",
            "[[ 427  217  155]\n",
            " [1055 2115  612]\n",
            " [ 405  236  156]]\n",
            "Epoch 17: Train Loss: 1.1324 | Train Acc: 47.66%   Val Loss:   1.1588 | Val Acc:   53.96%\n",
            "Validation Confusion Matrix:\n",
            "[[ 387  296  116]\n",
            " [ 943 2388  451]\n",
            " [ 382  288  127]]\n",
            "Epoch 18: Train Loss: 1.1315 | Train Acc: 48.77%   Val Loss:   1.1479 | Val Acc:   43.19%\n",
            "Validation Confusion Matrix:\n",
            "[[ 531  153  115]\n",
            " [1458 1672  652]\n",
            " [ 522  155  120]]\n",
            "Epoch 19: Train Loss: 1.1295 | Train Acc: 48.07%   Val Loss:   1.1525 | Val Acc:   49.81%\n",
            "Validation Confusion Matrix:\n",
            "[[ 465  221  113]\n",
            " [1169 2078  535]\n",
            " [ 438  223  136]]\n",
            "Epoch 20: Train Loss: 1.1289 | Train Acc: 48.09%   Val Loss:   1.1471 | Val Acc:   52.60%\n",
            "Validation Confusion Matrix:\n",
            "[[ 363  256  180]\n",
            " [ 771 2266  745]\n",
            " [ 331  266  200]]\n",
            "Epoch 21: Train Loss: 1.1259 | Train Acc: 49.72%   Val Loss:   1.1468 | Val Acc:   48.77%\n",
            "Validation Confusion Matrix:\n",
            "[[ 372  223  204]\n",
            " [ 860 2041  881]\n",
            " [ 374  213  210]]\n",
            "Epoch 22: Train Loss: 1.1237 | Train Acc: 49.99%   Val Loss:   1.1677 | Val Acc:   52.60%\n",
            "Validation Confusion Matrix:\n",
            "[[ 392  266  141]\n",
            " [ 891 2257  634]\n",
            " [ 360  257  180]]\n",
            "Epoch 23: Train Loss: 1.1239 | Train Acc: 50.43%   Val Loss:   1.1705 | Val Acc:   58.18%\n",
            "Validation Confusion Matrix:\n",
            "[[ 315  341  143]\n",
            " [ 650 2658  474]\n",
            " [ 293  348  156]]\n",
            "âœ… Best model saved (58.18%)\n",
            "Epoch 24: Train Loss: 1.1215 | Train Acc: 49.99%   Val Loss:   1.1579 | Val Acc:   51.36%\n",
            "Validation Confusion Matrix:\n",
            "[[ 348  262  189]\n",
            " [ 777 2201  804]\n",
            " [ 339  245  213]]\n",
            "Epoch 25: Train Loss: 1.1181 | Train Acc: 50.27%   Val Loss:   1.1697 | Val Acc:   43.42%\n",
            "Validation Confusion Matrix:\n",
            "[[ 433  168  198]\n",
            " [1053 1725 1004]\n",
            " [ 435  185  177]]\n",
            "Epoch 26: Train Loss: 1.1184 | Train Acc: 50.40%   Val Loss:   1.1725 | Val Acc:   48.85%\n",
            "Validation Confusion Matrix:\n",
            "[[ 402  216  181]\n",
            " [ 942 2026  814]\n",
            " [ 375  223  199]]\n",
            "Epoch 27: Train Loss: 1.1163 | Train Acc: 50.72%   Val Loss:   1.1777 | Val Acc:   51.86%\n",
            "Validation Confusion Matrix:\n",
            "[[ 345  264  190]\n",
            " [ 769 2239  774]\n",
            " [ 320  272  205]]\n",
            "Epoch 28: Train Loss: 1.1137 | Train Acc: 50.95%   Val Loss:   1.1918 | Val Acc:   54.54%\n",
            "Validation Confusion Matrix:\n",
            "[[ 230  315  254]\n",
            " [ 437 2415  930]\n",
            " [ 215  294  288]]\n",
            "Epoch 29: Train Loss: 1.1134 | Train Acc: 51.03%   Val Loss:   1.1994 | Val Acc:   51.64%\n",
            "Validation Confusion Matrix:\n",
            "[[ 357  253  189]\n",
            " [ 780 2229  773]\n",
            " [ 340  266  191]]\n",
            "Epoch 30: Train Loss: 1.1116 | Train Acc: 51.15%   Val Loss:   1.1634 | Val Acc:   46.65%\n",
            "Validation Confusion Matrix:\n",
            "[[ 320  219  260]\n",
            " [ 696 1900 1186]\n",
            " [ 310  198  289]]\n",
            "Epoch 31: Train Loss: 1.1082 | Train Acc: 51.56%   Val Loss:   1.1942 | Val Acc:   50.00%\n",
            "Validation Confusion Matrix:\n",
            "[[ 309  232  258]\n",
            " [ 637 2097 1048]\n",
            " [ 283  231  283]]\n",
            "Epoch 32: Train Loss: 1.1074 | Train Acc: 51.57%   Val Loss:   1.2146 | Val Acc:   51.75%\n",
            "Validation Confusion Matrix:\n",
            "[[ 390  250  159]\n",
            " [ 944 2240  598]\n",
            " [ 380  264  153]]\n",
            "Epoch 33: Train Loss: 1.1064 | Train Acc: 51.57%   Val Loss:   1.1802 | Val Acc:   40.91%\n",
            "Validation Confusion Matrix:\n",
            "[[ 471  138  190]\n",
            " [1214 1543 1025]\n",
            " [ 453  158  186]]\n",
            "Epoch 34: Train Loss: 1.1041 | Train Acc: 52.03%   Val Loss:   1.2151 | Val Acc:   51.19%\n",
            "Validation Confusion Matrix:\n",
            "[[ 331  263  205]\n",
            " [ 698 2184  900]\n",
            " [ 291  268  238]]\n",
            "Epoch 35: Train Loss: 1.1029 | Train Acc: 51.66%   Val Loss:   1.1819 | Val Acc:   39.87%\n",
            "Validation Confusion Matrix:\n",
            "[[ 450  151  198]\n",
            " [1213 1495 1074]\n",
            " [ 437  161  199]]\n",
            "Epoch 36: Train Loss: 1.1015 | Train Acc: 52.74%   Val Loss:   1.2061 | Val Acc:   42.82%\n",
            "Validation Confusion Matrix:\n",
            "[[ 427  170  202]\n",
            " [1133 1686  963]\n",
            " [ 414  193  190]]\n",
            "Epoch 37: Train Loss: 1.0988 | Train Acc: 51.92%   Val Loss:   1.1977 | Val Acc:   49.20%\n",
            "Validation Confusion Matrix:\n",
            "[[ 224  224  351]\n",
            " [ 429 2056 1297]\n",
            " [ 197  234  366]]\n",
            "Epoch 38: Train Loss: 1.0951 | Train Acc: 52.83%   Val Loss:   1.2254 | Val Acc:   52.18%\n",
            "Validation Confusion Matrix:\n",
            "[[ 265  283  251]\n",
            " [ 578 2264  940]\n",
            " [ 234  286  277]]\n",
            "Epoch 39: Train Loss: 1.0931 | Train Acc: 53.31%   Val Loss:   1.2208 | Val Acc:   51.19%\n",
            "Validation Confusion Matrix:\n",
            "[[ 408  276  115]\n",
            " [1059 2222  501]\n",
            " [ 407  267  123]]\n",
            "Epoch 40: Train Loss: 1.0900 | Train Acc: 53.26%   Val Loss:   1.2521 | Val Acc:   52.36%\n",
            "Validation Confusion Matrix:\n",
            "[[ 277  288  234]\n",
            " [ 665 2278  839]\n",
            " [ 249  287  261]]\n",
            "Epoch 41: Train Loss: 1.0891 | Train Acc: 53.70%   Val Loss:   1.2172 | Val Acc:   52.92%\n",
            "Validation Confusion Matrix:\n",
            "[[ 275  275  249]\n",
            " [ 620 2308  854]\n",
            " [ 243  291  263]]\n",
            "Epoch 42: Train Loss: 1.0863 | Train Acc: 53.73%   Val Loss:   1.2217 | Val Acc:   48.62%\n",
            "Validation Confusion Matrix:\n",
            "[[ 402  239  158]\n",
            " [1047 2043  692]\n",
            " [ 370  257  170]]\n",
            "Epoch 43: Train Loss: 1.0827 | Train Acc: 53.90%   Val Loss:   1.2614 | Val Acc:   53.40%\n",
            "Validation Confusion Matrix:\n",
            "[[ 396  281  122]\n",
            " [1008 2355  419]\n",
            " [ 378  298  121]]\n",
            "Epoch 44: Train Loss: 1.0787 | Train Acc: 54.17%   Val Loss:   1.2570 | Val Acc:   49.20%\n",
            "Validation Confusion Matrix:\n",
            "[[ 352  247  200]\n",
            " [ 893 2084  805]\n",
            " [ 337  250  210]]\n",
            "Epoch 45: Train Loss: 1.0775 | Train Acc: 54.57%   Val Loss:   1.3000 | Val Acc:   57.55%\n",
            "Validation Confusion Matrix:\n",
            "[[ 217  368  214]\n",
            " [ 478 2635  669]\n",
            " [ 178  376  243]]\n",
            "Epoch 46: Train Loss: 1.0729 | Train Acc: 54.68%   Val Loss:   1.2583 | Val Acc:   47.60%\n",
            "Validation Confusion Matrix:\n",
            "[[ 440  232  127]\n",
            " [1214 1982  586]\n",
            " [ 441  218  138]]\n",
            "Epoch 47: Train Loss: 1.0705 | Train Acc: 55.85%   Val Loss:   1.2410 | Val Acc:   48.38%\n",
            "Validation Confusion Matrix:\n",
            "[[ 432  221  146]\n",
            " [1143 2001  638]\n",
            " [ 399  229  169]]\n",
            "Epoch 48: Train Loss: 1.0696 | Train Acc: 55.42%   Val Loss:   1.2750 | Val Acc:   52.06%\n",
            "Validation Confusion Matrix:\n",
            "[[ 384  267  148]\n",
            " [ 977 2251  554]\n",
            " [ 358  274  165]]\n",
            "Epoch 49: Train Loss: 1.0646 | Train Acc: 55.43%   Val Loss:   1.2959 | Val Acc:   53.12%\n",
            "Validation Confusion Matrix:\n",
            "[[ 209  290  300]\n",
            " [ 493 2335  954]\n",
            " [ 182  302  313]]\n",
            "Epoch 50: Train Loss: 1.0633 | Train Acc: 55.59%   Val Loss:   1.2699 | Val Acc:   47.88%\n",
            "Validation Confusion Matrix:\n",
            "[[ 429  232  138]\n",
            " [1199 1984  599]\n",
            " [ 416  219  162]]\n",
            "Epoch 51: Train Loss: 1.0607 | Train Acc: 55.92%   Val Loss:   1.3314 | Val Acc:   50.69%\n",
            "Validation Confusion Matrix:\n",
            "[[ 282  276  241]\n",
            " [ 763 2171  848]\n",
            " [ 240  284  273]]\n",
            "Epoch 52: Train Loss: 1.0582 | Train Acc: 56.34%   Val Loss:   1.2748 | Val Acc:   45.61%\n",
            "Validation Confusion Matrix:\n",
            "[[ 510  199   90]\n",
            " [1471 1824  487]\n",
            " [ 468  210  119]]\n",
            "Epoch 53: Train Loss: 1.0542 | Train Acc: 56.26%   Val Loss:   1.3069 | Val Acc:   45.15%\n",
            "Validation Confusion Matrix:\n",
            "[[ 329  198  272]\n",
            " [ 811 1779 1192]\n",
            " [ 277  200  320]]\n",
            "Epoch 54: Train Loss: 1.0525 | Train Acc: 56.95%   Val Loss:   1.2790 | Val Acc:   44.29%\n",
            "Validation Confusion Matrix:\n",
            "[[ 508  189  102]\n",
            " [1458 1743  581]\n",
            " [ 469  197  131]]\n",
            "Epoch 55: Train Loss: 1.0475 | Train Acc: 56.84%   Val Loss:   1.2782 | Val Acc:   50.89%\n",
            "Validation Confusion Matrix:\n",
            "[[ 348  266  185]\n",
            " [ 886 2182  714]\n",
            " [ 308  282  207]]\n",
            "Epoch 56: Train Loss: 1.0443 | Train Acc: 57.29%   Val Loss:   1.2943 | Val Acc:   50.04%\n",
            "Validation Confusion Matrix:\n",
            "[[ 434  241  124]\n",
            " [1180 2114  488]\n",
            " [ 400  254  143]]\n",
            "Epoch 57: Train Loss: 1.0404 | Train Acc: 57.73%   Val Loss:   1.2807 | Val Acc:   50.04%\n",
            "Validation Confusion Matrix:\n",
            "[[ 366  243  190]\n",
            " [ 940 2121  721]\n",
            " [ 337  256  204]]\n",
            "Epoch 58: Train Loss: 1.0415 | Train Acc: 57.24%   Val Loss:   1.3015 | Val Acc:   49.93%\n",
            "Validation Confusion Matrix:\n",
            "[[ 382  246  171]\n",
            " [1033 2097  652]\n",
            " [ 334  257  206]]\n",
            "Epoch 59: Train Loss: 1.0370 | Train Acc: 57.21%   Val Loss:   1.2769 | Val Acc:   51.12%\n",
            "Validation Confusion Matrix:\n",
            "[[ 415  258  126]\n",
            " [1108 2169  505]\n",
            " [ 367  265  165]]\n",
            "Epoch 60: Train Loss: 1.0345 | Train Acc: 57.65%   Val Loss:   1.3515 | Val Acc:   53.07%\n",
            "Validation Confusion Matrix:\n",
            "[[ 312  290  197]\n",
            " [ 796 2306  680]\n",
            " [ 275  286  236]]\n",
            "Epoch 61: Train Loss: 1.0306 | Train Acc: 58.42%   Val Loss:   1.3159 | Val Acc:   47.62%\n",
            "Validation Confusion Matrix:\n",
            "[[ 426  214  159]\n",
            " [1206 1943  633]\n",
            " [ 376  229  192]]\n",
            "Epoch 62: Train Loss: 1.0282 | Train Acc: 57.88%   Val Loss:   1.3098 | Val Acc:   46.67%\n",
            "Validation Confusion Matrix:\n",
            "[[ 427  196  176]\n",
            " [1219 1888  675]\n",
            " [ 386  216  195]]\n",
            "Epoch 63: Train Loss: 1.0265 | Train Acc: 58.06%   Val Loss:   1.3243 | Val Acc:   49.70%\n",
            "Validation Confusion Matrix:\n",
            "[[ 369  240  190]\n",
            " [1019 2074  689]\n",
            " [ 317  250  230]]\n",
            "Epoch 64: Train Loss: 1.0239 | Train Acc: 57.98%   Val Loss:   1.3085 | Val Acc:   47.01%\n",
            "Validation Confusion Matrix:\n",
            "[[ 379  224  196]\n",
            " [1087 1908  787]\n",
            " [ 352  204  241]]\n",
            "Epoch 65: Train Loss: 1.0191 | Train Acc: 58.72%   Val Loss:   1.3195 | Val Acc:   51.30%\n",
            "Validation Confusion Matrix:\n",
            "[[ 389  246  164]\n",
            " [1105 2173  504]\n",
            " [ 342  258  197]]\n",
            "Epoch 66: Train Loss: 1.0169 | Train Acc: 58.94%   Val Loss:   1.3479 | Val Acc:   58.39%\n",
            "Validation Confusion Matrix:\n",
            "[[ 263  375  161]\n",
            " [ 633 2679  470]\n",
            " [ 225  374  198]]\n",
            "âœ… Best model saved (58.39%)\n",
            "Epoch 67: Train Loss: 1.0167 | Train Acc: 58.46%   Val Loss:   1.3555 | Val Acc:   46.56%\n",
            "Validation Confusion Matrix:\n",
            "[[ 401  214  184]\n",
            " [1228 1884  670]\n",
            " [ 360  218  219]]\n",
            "Epoch 68: Train Loss: 1.0102 | Train Acc: 59.81%   Val Loss:   1.3493 | Val Acc:   47.10%\n",
            "Validation Confusion Matrix:\n",
            "[[ 387  211  201]\n",
            " [1157 1913  712]\n",
            " [ 338  226  233]]\n",
            "Epoch 69: Train Loss: 1.0093 | Train Acc: 59.24%   Val Loss:   1.3329 | Val Acc:   49.89%\n",
            "Validation Confusion Matrix:\n",
            "[[ 440  243  116]\n",
            " [1214 2100  468]\n",
            " [ 389  265  143]]\n",
            "Epoch 70: Train Loss: 1.0033 | Train Acc: 59.67%   Val Loss:   1.3955 | Val Acc:   55.08%\n",
            "Validation Confusion Matrix:\n",
            "[[ 347  326  126]\n",
            " [ 869 2460  453]\n",
            " [ 316  326  155]]\n",
            "Epoch 71: Train Loss: 1.0031 | Train Acc: 60.38%   Val Loss:   1.3613 | Val Acc:   46.45%\n",
            "Validation Confusion Matrix:\n",
            "[[ 436  210  153]\n",
            " [1269 1875  638]\n",
            " [ 391  219  187]]\n",
            "Epoch 72: Train Loss: 0.9997 | Train Acc: 60.12%   Val Loss:   1.4064 | Val Acc:   51.04%\n",
            "Validation Confusion Matrix:\n",
            "[[ 234  262  303]\n",
            " [ 600 2150 1032]\n",
            " [ 177  259  361]]\n",
            "Epoch 73: Train Loss: 0.9947 | Train Acc: 60.25%   Val Loss:   1.3428 | Val Acc:   45.54%\n",
            "Validation Confusion Matrix:\n",
            "[[ 440  196  163]\n",
            " [1255 1792  735]\n",
            " [ 362  218  217]]\n",
            "Epoch 74: Train Loss: 0.9934 | Train Acc: 60.42%   Val Loss:   1.4251 | Val Acc:   54.44%\n",
            "Validation Confusion Matrix:\n",
            "[[ 220  328  251]\n",
            " [ 578 2415  789]\n",
            " [ 179  325  293]]\n",
            "Epoch 75: Train Loss: 0.9893 | Train Acc: 61.07%   Val Loss:   1.3610 | Val Acc:   50.69%\n",
            "Validation Confusion Matrix:\n",
            "[[ 452  257   90]\n",
            " [1283 2157  342]\n",
            " [ 401  279  117]]\n",
            "Epoch 76: Train Loss: 0.9875 | Train Acc: 60.95%   Val Loss:   1.4096 | Val Acc:   49.48%\n",
            "Validation Confusion Matrix:\n",
            "[[ 394  270  135]\n",
            " [1136 2071  575]\n",
            " [ 331  270  196]]\n",
            "Epoch 77: Train Loss: 0.9847 | Train Acc: 60.64%   Val Loss:   1.3352 | Val Acc:   52.06%\n",
            "Validation Confusion Matrix:\n",
            "[[ 316  293  190]\n",
            " [ 880 2253  649]\n",
            " [ 275  291  231]]\n",
            "Epoch 78: Train Loss: 0.9831 | Train Acc: 60.90%   Val Loss:   1.3458 | Val Acc:   53.83%\n",
            "Validation Confusion Matrix:\n",
            "[[ 352  303  144]\n",
            " [ 895 2328  559]\n",
            " [ 274  308  215]]\n",
            "Epoch 79: Train Loss: 0.9791 | Train Acc: 61.53%   Val Loss:   1.3705 | Val Acc:   52.47%\n",
            "Validation Confusion Matrix:\n",
            "[[ 357  271  171]\n",
            " [ 903 2258  621]\n",
            " [ 318  272  207]]\n",
            "Epoch 80: Train Loss: 0.9747 | Train Acc: 62.03%   Val Loss:   1.3894 | Val Acc:   55.15%\n",
            "Validation Confusion Matrix:\n",
            "[[ 400  309   90]\n",
            " [1009 2434  339]\n",
            " [ 349  316  132]]\n",
            "Epoch 81: Train Loss: 0.9755 | Train Acc: 61.31%   Val Loss:   1.3957 | Val Acc:   53.83%\n",
            "Validation Confusion Matrix:\n",
            "[[ 309  293  197]\n",
            " [ 754 2337  691]\n",
            " [ 244  304  249]]\n",
            "Epoch 82: Train Loss: 0.9723 | Train Acc: 61.99%   Val Loss:   1.3521 | Val Acc:   48.77%\n",
            "Validation Confusion Matrix:\n",
            "[[ 394  214  191]\n",
            " [1046 1986  750]\n",
            " [ 324  230  243]]\n",
            "Epoch 83: Train Loss: 0.9694 | Train Acc: 62.06%   Val Loss:   1.3580 | Val Acc:   54.72%\n",
            "Validation Confusion Matrix:\n",
            "[[ 303  307  189]\n",
            " [ 787 2402  593]\n",
            " [ 247  312  238]]\n",
            "Epoch 84: Train Loss: 0.9678 | Train Acc: 62.10%   Val Loss:   1.4092 | Val Acc:   53.25%\n",
            "Validation Confusion Matrix:\n",
            "[[ 273  297  229]\n",
            " [ 705 2286  791]\n",
            " [ 205  287  305]]\n",
            "Epoch 85: Train Loss: 0.9636 | Train Acc: 62.36%   Val Loss:   1.3769 | Val Acc:   53.66%\n",
            "Validation Confusion Matrix:\n",
            "[[ 422  300   77]\n",
            " [1135 2346  301]\n",
            " [ 368  311  118]]\n",
            "Epoch 86: Train Loss: 0.9599 | Train Acc: 62.52%   Val Loss:   1.3428 | Val Acc:   49.00%\n",
            "Validation Confusion Matrix:\n",
            "[[ 408  226  165]\n",
            " [1165 2003  614]\n",
            " [ 332  241  224]]\n",
            "Epoch 87: Train Loss: 0.9600 | Train Acc: 62.44%   Val Loss:   1.3838 | Val Acc:   55.09%\n",
            "Validation Confusion Matrix:\n",
            "[[ 340  315  144]\n",
            " [ 858 2436  488]\n",
            " [ 296  314  187]]\n",
            "Epoch 88: Train Loss: 0.9548 | Train Acc: 63.03%   Val Loss:   1.4467 | Val Acc:   50.99%\n",
            "Validation Confusion Matrix:\n",
            "[[ 342  282  175]\n",
            " [ 926 2179  677]\n",
            " [ 301  275  221]]\n",
            "Epoch 89: Train Loss: 0.9546 | Train Acc: 62.73%   Val Loss:   1.4223 | Val Acc:   52.19%\n",
            "Validation Confusion Matrix:\n",
            "[[ 333  300  166]\n",
            " [ 963 2259  560]\n",
            " [ 283  299  215]]\n",
            "Epoch 90: Train Loss: 0.9519 | Train Acc: 63.11%   Val Loss:   1.3502 | Val Acc:   49.13%\n",
            "Validation Confusion Matrix:\n",
            "[[ 359  257  183]\n",
            " [1099 2047  636]\n",
            " [ 303  258  236]]\n",
            "Epoch 91: Train Loss: 0.9443 | Train Acc: 63.41%   Val Loss:   1.4221 | Val Acc:   50.97%\n",
            "Validation Confusion Matrix:\n",
            "[[ 349  280  170]\n",
            " [1022 2183  577]\n",
            " [ 295  293  209]]\n",
            "Epoch 92: Train Loss: 0.9450 | Train Acc: 63.07%   Val Loss:   1.4294 | Val Acc:   59.82%\n",
            "Validation Confusion Matrix:\n",
            "[[ 256  396  147]\n",
            " [ 596 2790  396]\n",
            " [ 221  405  171]]\n",
            "âœ… Best model saved (59.82%)\n",
            "Epoch 93: Train Loss: 0.9459 | Train Acc: 63.70%   Val Loss:   1.3930 | Val Acc:   50.46%\n",
            "Validation Confusion Matrix:\n",
            "[[ 420  261  118]\n",
            " [1216 2145  421]\n",
            " [ 382  266  149]]\n",
            "Epoch 94: Train Loss: 0.9393 | Train Acc: 63.64%   Val Loss:   1.4082 | Val Acc:   58.31%\n",
            "Validation Confusion Matrix:\n",
            "[[ 260  406  133]\n",
            " [ 705 2703  374]\n",
            " [ 221  403  173]]\n",
            "Epoch 95: Train Loss: 0.9344 | Train Acc: 64.22%   Val Loss:   1.4048 | Val Acc:   50.86%\n",
            "Validation Confusion Matrix:\n",
            "[[ 303  263  233]\n",
            " [ 851 2143  788]\n",
            " [ 238  270  289]]\n",
            "Epoch 96: Train Loss: 0.9377 | Train Acc: 64.24%   Val Loss:   1.3672 | Val Acc:   54.33%\n",
            "Validation Confusion Matrix:\n",
            "[[ 377  316  106]\n",
            " [1001 2394  387]\n",
            " [ 318  328  151]]\n",
            "Epoch 97: Train Loss: 0.9303 | Train Acc: 64.39%   Val Loss:   1.4115 | Val Acc:   45.85%\n",
            "Validation Confusion Matrix:\n",
            "[[ 390  223  186]\n",
            " [1284 1851  647]\n",
            " [ 342  230  225]]\n",
            "Epoch 98: Train Loss: 0.9304 | Train Acc: 64.43%   Val Loss:   1.3599 | Val Acc:   50.17%\n",
            "Validation Confusion Matrix:\n",
            "[[ 372  267  160]\n",
            " [1053 2117  612]\n",
            " [ 327  261  209]]\n",
            "Epoch 99: Train Loss: 0.9262 | Train Acc: 64.62%   Val Loss:   1.4169 | Val Acc:   47.66%\n",
            "Validation Confusion Matrix:\n",
            "[[ 340  239  220]\n",
            " [1070 1968  744]\n",
            " [ 286  256  255]]\n",
            "Epoch 100: Train Loss: 0.9202 | Train Acc: 65.64%   Val Loss:   1.3950 | Val Acc:   55.26%\n",
            "Validation Confusion Matrix:\n",
            "[[ 291  344  164]\n",
            " [ 817 2472  493]\n",
            " [ 247  341  209]]\n",
            "\n",
            "ğŸ Training Finished! Best Val Acc = 59.82%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# ======= 1ï¸âƒ£ è¶…å‚æ•°é…ç½® =======\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_classes = 3\n",
        "lr = 1e-4\n",
        "num_epochs = 100\n",
        "save_path = \"best_model.pt\"\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "print(\"âš–ï¸ è®¡ç®—ç±»åˆ«æƒé‡...\")\n",
        "# 1. ç»Ÿè®¡è®­ç»ƒé›†ä¸­æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°\n",
        "train_labels = [s['label'].item() for s in train_samples]\n",
        "label_counts = Counter(sorted(train_labels))\n",
        "\n",
        "# 2. è®¡ç®—æƒé‡ (ä½¿ç”¨æ ‡å‡†çš„åæ¯”é¢‘ç‡å…¬å¼)\n",
        "#    å…¬å¼: weight[c] = total_samples / (num_classes * count[c])\n",
        "total_samples = len(train_labels)\n",
        "weights = [total_samples / (len(label_counts) * label_counts[i]) for i in sorted(label_counts.keys())]\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "\n",
        "print(f\"ç±»åˆ«è®¡æ•°: {label_counts}\")\n",
        "print(f\"è®¡ç®—å‡ºçš„ç±»åˆ«æƒé‡: {class_weights}\")\n",
        "\n",
        "# ======= 2ï¸âƒ£ æ¨¡å‹å®ä¾‹åŒ– =======\n",
        "model = MultiModalTransformer(\n",
        "    stock_dim=6,\n",
        "    emo_dim=10,\n",
        "    text_dim=2048,\n",
        "    proj_dim=128,\n",
        "    hidden_dim=128,\n",
        "    num_heads=4,\n",
        "    num_layers=2,\n",
        "    num_classes=num_classes,\n",
        "    max_len=512,\n",
        "    dropout=0.5\n",
        ").to(device)\n",
        "\n",
        "# ======= 3ï¸âƒ£ ä¼˜åŒ–å™¨ + æŸå¤±å‡½æ•° =======\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "# ======= 4ï¸âƒ£ è®­ç»ƒä¸éªŒè¯å¾ªç¯ =======\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # print(f\"\\nğŸŸ¢ Epoch {epoch}/{num_epochs}\")\n",
        "    # === Train ===\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        emo = batch[\"emotion\"].to(device)\n",
        "        stk = batch[\"stock\"].to(device)\n",
        "        txt = batch[\"text\"].to(device)\n",
        "        y = batch[\"label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(stk, emo, txt)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * y.size(0)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "    train_loss /= total\n",
        "    # scheduler.step()\n",
        "\n",
        "    # === Validate ===\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            emo = batch[\"emotion\"].to(device)\n",
        "            stk = batch[\"stock\"].to(device)\n",
        "            txt = batch[\"text\"].to(device)\n",
        "            y = batch[\"label\"].to(device)\n",
        "\n",
        "            logits = model(stk, emo, txt)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "            val_loss += loss.item() * y.size(0)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            val_correct += (pred == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "            all_val_preds.extend(pred.cpu().numpy())\n",
        "            all_val_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\", end = \"   \")\n",
        "    print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc*100:.2f}%\")\n",
        "\n",
        "    cm = confusion_matrix(all_val_labels, all_val_preds)\n",
        "    print(\"Validation Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # === Save best model ===\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"âœ… Best model saved ({best_val_acc*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nğŸ Training Finished! Best Val Acc = {best_val_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a0d4a1",
      "metadata": {
        "id": "70a0d4a1"
      },
      "source": [
        "# Transformer (2 multimodal input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e20a054e",
      "metadata": {
        "id": "e20a054e",
        "outputId": "3e5246f1-ffe9-40b4-babc-3358077cd2b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¥ è¯»å–æ•°æ®ä¸­...\n",
            "âœ… å…± 55381 è¡Œ, 2073 åˆ—\n",
            "ğŸ§© ç‰¹å¾æ€»æ•°: 6\n",
            "âœ… å…±ç”Ÿæˆ 53081 ä¸ªçª—å£æ ·æœ¬\n",
            "âš ï¸ è­¦å‘Šï¼šæ­£åœ¨ä½¿ç”¨éšæœºåˆ’åˆ†ã€‚\n",
            "ğŸ“Š è®­ç»ƒé›† 42464ï¼ŒéªŒè¯é›† 10617\n",
            "âœ… DataLoader æ„å»ºå®Œæˆ\n",
            "emotion: torch.Size([64, 10, 4])\n",
            "stock: torch.Size([64, 10, 2])\n",
            "label: torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ===== å‚æ•° =====\n",
        "path = \"merged.parquet\"\n",
        "window_size = 10\n",
        "test_ratio = 0.2\n",
        "batch_size = 64\n",
        "\n",
        "# ===== 1ï¸âƒ£ è¯»å– parquet =====\n",
        "print(\"ğŸ“¥ è¯»å–æ•°æ®ä¸­...\")\n",
        "df = pd.read_parquet(path)\n",
        "df = df.sort_values([\"StockCode\", \"Date\"]).reset_index(drop=True)\n",
        "print(f\"âœ… å…± {len(df)} è¡Œ, {df.shape[1]} åˆ—\")\n",
        "\n",
        "# ===== 2ï¸âƒ£ ç‰¹å¾åˆ†ç±» (ä¸ä½¿ç”¨ text_cols) =====\n",
        "emotion_cols = ['Emotion_Index_closing',\n",
        " 'Emotion_Index_trading',\n",
        " 'Emotion_Momentum_3d_closing',\n",
        " 'Emotion_Momentum_3d_trading']\n",
        "stock_cols = ['log_return', 'amplitude']\n",
        "\n",
        "feature_cols = emotion_cols + stock_cols\n",
        "print(f\"ğŸ§© ç‰¹å¾æ€»æ•°: {len(feature_cols)}\")\n",
        "\n",
        "# ===== 3ï¸âƒ£ æ„é€ æ‰€æœ‰çª—å£æ ·æœ¬ =====\n",
        "samples = []\n",
        "\n",
        "for code, group in df.groupby(\"StockCode\"):\n",
        "    group = group.reset_index(drop=True)\n",
        "    if len(group) <= window_size:\n",
        "        continue\n",
        "\n",
        "    # åªæå– emotion å’Œ stock ç‰¹å¾\n",
        "    emo = torch.tensor(group[emotion_cols].values, dtype=torch.float32)\n",
        "    stk = torch.tensor(group[stock_cols].values, dtype=torch.float32)\n",
        "    label = torch.tensor(group[\"Label\"].values, dtype=torch.int64)\n",
        "    date = group[\"Date\"].values\n",
        "\n",
        "    for i in range(len(group) - window_size):\n",
        "        samples.append({\n",
        "            \"StockCode\": code,\n",
        "            \"end_date\": date[i + window_size],\n",
        "            \"emotion\": emo[i:i + window_size],\n",
        "            \"stock\": stk[i:i + window_size],\n",
        "            \"label\": label[i + window_size]\n",
        "        })\n",
        "\n",
        "print(f\"âœ… å…±ç”Ÿæˆ {len(samples)} ä¸ªçª—å£æ ·æœ¬\")\n",
        "\n",
        "# ===== 4ï¸âƒ£ éšæœºåˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† (æ ¹æ®æ‚¨çš„è¦æ±‚) =====\n",
        "# è­¦å‘Šï¼šå¯¹äºæ—¶åºæ•°æ®ï¼Œéšæœºåˆ’åˆ†å¯èƒ½å¯¼è‡´æ•°æ®æ³„éœ²å’Œè¯„ä¼°ç»“æœè™šé«˜ã€‚\n",
        "print(\"âš ï¸ è­¦å‘Šï¼šæ­£åœ¨ä½¿ç”¨éšæœºåˆ’åˆ†ã€‚\")\n",
        "train_samples, val_samples = train_test_split(samples, test_size=test_ratio, random_state=42, shuffle=True)\n",
        "print(f\"ğŸ“Š è®­ç»ƒé›† {len(train_samples)}ï¼ŒéªŒè¯é›† {len(val_samples)}\")\n",
        "\n",
        "# ===== 5ï¸âƒ£ å®šä¹‰ Dataset (æ–°åç§°: StockEmotionDataset) =====\n",
        "class StockEmotionDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        return {\n",
        "            \"emotion\": s[\"emotion\"],\n",
        "            \"stock\": s[\"stock\"],\n",
        "            \"label\": s[\"label\"]\n",
        "        }\n",
        "\n",
        "train_set = StockEmotionDataset(train_samples)\n",
        "val_set = StockEmotionDataset(val_samples)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "print(\"âœ… DataLoader æ„å»ºå®Œæˆ\")\n",
        "\n",
        "# ===== 6ï¸âƒ£ æ‰“å°ç»´åº¦æ£€æŸ¥ =====\n",
        "for batch in train_loader:\n",
        "    print(\"emotion:\", batch[\"emotion\"].shape)\n",
        "    print(\"stock:\", batch[\"stock\"].shape)\n",
        "    print(\"label:\", batch[\"label\"].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df2fde28",
      "metadata": {
        "id": "df2fde28"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    å›ºå®šçš„æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç æ¨¡å—ã€‚\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, dim)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (Batch, SeqLen, Dim)\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "class TwoModalTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    ä¸€ä¸ªæ›´ç²¾ç®€çš„ Transformer æ¨¡å‹ï¼Œç”¨äºå¤„ç† stock å’Œ emotion ä¸¤ç§æ¨¡æ€æ•°æ®ã€‚\n",
        "    - ä½¿ç”¨å›ºå®šçš„æ­£ä½™å¼¦ä½ç½®ç¼–ç ã€‚\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        stock_dim: int = 6,\n",
        "        emo_dim: int = 10,\n",
        "        proj_dim: int = 64,\n",
        "        hidden_dim: int = 256,\n",
        "        num_heads: int = 4,\n",
        "        num_layers: int = 2,\n",
        "        num_classes: int = 3,\n",
        "        max_len: int = 512,\n",
        "        dropout: float = 0.2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # === å„æ¨¡æ€ç¼–ç  ===\n",
        "        self.stock_proj = nn.Sequential(\n",
        "            nn.LayerNorm(stock_dim),\n",
        "            nn.Linear(stock_dim, proj_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(proj_dim)\n",
        "        )\n",
        "\n",
        "        self.emo_proj = nn.Sequential(\n",
        "            nn.LayerNorm(emo_dim),\n",
        "            nn.Linear(emo_dim, proj_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(proj_dim)\n",
        "        )\n",
        "\n",
        "        # === èåˆåˆ° hidden_dim ===\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(proj_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.LayerNorm(hidden_dim)\n",
        "        )\n",
        "\n",
        "        # === ä½ç½®ç¼–ç  (ä½¿ç”¨å›ºå®šçš„æ­£ä½™å¼¦ç¼–ç ) ===\n",
        "        self.pos_encoder = PositionalEncoding(dim=hidden_dim, max_len=max_len)\n",
        "\n",
        "        # === Transformer ç¼–ç å™¨ ===\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            activation=\"relu\",\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True,\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # === æ—¶é—´æ³¨æ„åŠ›æ± åŒ– ===\n",
        "        self.time_attn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        # === åˆ†ç±»å¤´ ===\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, stock_seq, emo_seq):\n",
        "        \"\"\"\n",
        "        è¾“å…¥ï¼š\n",
        "            stock_seq: (B, T, stock_dim)\n",
        "            emo_seq:   (B, T, emo_dim)\n",
        "        è¾“å‡ºï¼š\n",
        "            logits:    (B, num_classes)\n",
        "        \"\"\"\n",
        "        # 1) å„æ¨¡æ€ç‰¹å¾åˆ°åŒä¸€å°ºåº¦\n",
        "        s = self.stock_proj(stock_seq) # (B, T, proj_dim)\n",
        "        e = self.emo_proj(emo_seq)     # (B, T, proj_dim)\n",
        "\n",
        "        # 2) èåˆ\n",
        "        x = torch.cat([s, e], dim=-1)   # (B, T, 2*proj_dim)\n",
        "        x = self.fuse(x)               # (B, T, hidden_dim)\n",
        "\n",
        "        # 3) ä½ç½®ç¼–ç \n",
        "        x = self.pos_encoder(x) # <--- ä¿®æ”¹åœ¨è¿™é‡Œ\n",
        "\n",
        "        # 4) æ—¶é—´ä¾èµ–ç¼–ç \n",
        "        enc_out = self.encoder(x)\n",
        "\n",
        "        # 5) æ³¨æ„åŠ›æ± åŒ–\n",
        "        attn = self.time_attn(enc_out)\n",
        "        attn = torch.softmax(attn, dim=1)\n",
        "        pooled = torch.sum(enc_out * attn, dim=1)\n",
        "\n",
        "        # 6) åˆ†ç±»\n",
        "        logits = self.cls_head(pooled)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16b6d3cc",
      "metadata": {
        "id": "16b6d3cc",
        "outputId": "a032467b-7738-4c4f-e039-9b978b3820bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "âš–ï¸ è®¡ç®—ç±»åˆ«æƒé‡...\n",
            "ç±»åˆ«è®¡æ•°: Counter({0: 16649, 2: 15614, 1: 10201})\n",
            "è®¡ç®—å‡ºçš„ç±»åˆ«æƒé‡: tensor([0.8502, 1.3876, 0.9065], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/msai/liny0145/.conda/envs/py312/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 1.1002 | Train Acc: 35.20%   Val Loss:   1.0960 | Val Acc:   31.34%\n",
            "Validation Confusion Matrix:\n",
            "[[1261 2667  336]\n",
            " [ 598 1767  163]\n",
            " [1089 2437  299]]\n",
            "âœ… Best model saved with Val Acc: 0.3134\n",
            "Epoch 2: Train Loss: 1.0955 | Train Acc: 34.86%   Val Loss:   1.0940 | Val Acc:   33.77%\n",
            "Validation Confusion Matrix:\n",
            "[[1762 2124  378]\n",
            " [ 850 1484  194]\n",
            " [1531 1955  339]]\n",
            "âœ… Best model saved with Val Acc: 0.3377\n",
            "Epoch 3: Train Loss: 1.0934 | Train Acc: 35.21%   Val Loss:   1.0914 | Val Acc:   35.92%\n",
            "Validation Confusion Matrix:\n",
            "[[1881 1871  512]\n",
            " [ 845 1369  314]\n",
            " [1575 1686  564]]\n",
            "âœ… Best model saved with Val Acc: 0.3592\n",
            "Epoch 4: Train Loss: 1.0917 | Train Acc: 36.01%   Val Loss:   1.0931 | Val Acc:   32.80%\n",
            "Validation Confusion Matrix:\n",
            "[[1492 2514  258]\n",
            " [ 655 1743  130]\n",
            " [1298 2280  247]]\n",
            "Epoch 5: Train Loss: 1.0910 | Train Acc: 36.12%   Val Loss:   1.0914 | Val Acc:   35.07%\n",
            "Validation Confusion Matrix:\n",
            "[[1596 1836  832]\n",
            " [ 736 1360  432]\n",
            " [1377 1681  767]]\n",
            "Epoch 6: Train Loss: 1.0897 | Train Acc: 35.95%   Val Loss:   1.0918 | Val Acc:   34.22%\n",
            "Validation Confusion Matrix:\n",
            "[[ 973 2080 1211]\n",
            " [ 408 1504  616]\n",
            " [ 797 1872 1156]]\n",
            "Epoch 7: Train Loss: 1.0898 | Train Acc: 36.12%   Val Loss:   1.0990 | Val Acc:   29.60%\n",
            "Validation Confusion Matrix:\n",
            "[[ 938 3263   63]\n",
            " [ 372 2117   39]\n",
            " [ 779 2958   88]]\n",
            "Epoch 8: Train Loss: 1.0884 | Train Acc: 36.26%   Val Loss:   1.0926 | Val Acc:   35.51%\n",
            "Validation Confusion Matrix:\n",
            "[[ 688 1211 2365]\n",
            " [ 326  963 1239]\n",
            " [ 572 1134 2119]]\n",
            "Epoch 9: Train Loss: 1.0875 | Train Acc: 36.45%   Val Loss:   1.0941 | Val Acc:   31.77%\n",
            "Validation Confusion Matrix:\n",
            "[[1093 2823  348]\n",
            " [ 433 1921  174]\n",
            " [ 899 2567  359]]\n",
            "Epoch 10: Train Loss: 1.0876 | Train Acc: 36.31%   Val Loss:   1.0911 | Val Acc:   34.46%\n",
            "Validation Confusion Matrix:\n",
            "[[1692 2214  358]\n",
            " [ 750 1584  194]\n",
            " [1436 2006  383]]\n",
            "Epoch 11: Train Loss: 1.0869 | Train Acc: 36.80%   Val Loss:   1.0905 | Val Acc:   35.57%\n",
            "Validation Confusion Matrix:\n",
            "[[1766 2039  459]\n",
            " [ 774 1498  256]\n",
            " [1458 1855  512]]\n",
            "Epoch 12: Train Loss: 1.0860 | Train Acc: 36.93%   Val Loss:   1.0911 | Val Acc:   34.90%\n",
            "Validation Confusion Matrix:\n",
            "[[ 606 1874 1784]\n",
            " [ 226 1418  884]\n",
            " [ 452 1692 1681]]\n",
            "Epoch 13: Train Loss: 1.0859 | Train Acc: 36.87%   Val Loss:   1.0969 | Val Acc:   38.33%\n",
            "Validation Confusion Matrix:\n",
            "[[1722  867 1675]\n",
            " [ 846  766  916]\n",
            " [1441  803 1581]]\n",
            "âœ… Best model saved with Val Acc: 0.3833\n",
            "Epoch 14: Train Loss: 1.0854 | Train Acc: 37.32%   Val Loss:   1.0896 | Val Acc:   37.37%\n",
            "Validation Confusion Matrix:\n",
            "[[1912 1462  890]\n",
            " [ 919 1167  442]\n",
            " [1596 1340  889]]\n",
            "Epoch 15: Train Loss: 1.0835 | Train Acc: 37.22%   Val Loss:   1.0909 | Val Acc:   35.69%\n",
            "Validation Confusion Matrix:\n",
            "[[1847 2014  403]\n",
            " [ 823 1482  223]\n",
            " [1561 1804  460]]\n",
            "Epoch 16: Train Loss: 1.0832 | Train Acc: 37.32%   Val Loss:   1.0925 | Val Acc:   33.82%\n",
            "Validation Confusion Matrix:\n",
            "[[ 369 1777 2118]\n",
            " [ 154 1325 1049]\n",
            " [ 319 1609 1897]]\n",
            "Epoch 17: Train Loss: 1.0835 | Train Acc: 37.29%   Val Loss:   1.0931 | Val Acc:   37.54%\n",
            "Validation Confusion Matrix:\n",
            "[[2303 1302  659]\n",
            " [1146 1038  344]\n",
            " [1968 1212  645]]\n",
            "Epoch 18: Train Loss: 1.0828 | Train Acc: 37.56%   Val Loss:   1.0920 | Val Acc:   35.77%\n",
            "Validation Confusion Matrix:\n",
            "[[1646 1770  848]\n",
            " [ 800 1328  400]\n",
            " [1394 1607  824]]\n",
            "Epoch 19: Train Loss: 1.0811 | Train Acc: 37.88%   Val Loss:   1.0924 | Val Acc:   35.05%\n",
            "Validation Confusion Matrix:\n",
            "[[1296 2126  842]\n",
            " [ 576 1539  413]\n",
            " [1098 1841  886]]\n",
            "Epoch 20: Train Loss: 1.0804 | Train Acc: 38.18%   Val Loss:   1.0958 | Val Acc:   34.04%\n",
            "Validation Confusion Matrix:\n",
            "[[1249 2306  709]\n",
            " [ 517 1668  343]\n",
            " [1053 2075  697]]\n",
            "Epoch 21: Train Loss: 1.0809 | Train Acc: 38.09%   Val Loss:   1.0930 | Val Acc:   35.72%\n",
            "Validation Confusion Matrix:\n",
            "[[1438 1873  953]\n",
            " [ 652 1402  474]\n",
            " [1200 1673  952]]\n",
            "Epoch 22: Train Loss: 1.0792 | Train Acc: 38.10%   Val Loss:   1.0939 | Val Acc:   35.35%\n",
            "Validation Confusion Matrix:\n",
            "[[1483 1928  853]\n",
            " [ 664 1410  454]\n",
            " [1251 1714  860]]\n",
            "Epoch 23: Train Loss: 1.0783 | Train Acc: 38.35%   Val Loss:   1.0983 | Val Acc:   34.05%\n",
            "Validation Confusion Matrix:\n",
            "[[ 932 2224 1108]\n",
            " [ 386 1623  519]\n",
            " [ 747 2018 1060]]\n",
            "Epoch 24: Train Loss: 1.0778 | Train Acc: 38.46%   Val Loss:   1.0955 | Val Acc:   36.54%\n",
            "Validation Confusion Matrix:\n",
            "[[1779 1685  800]\n",
            " [ 814 1282  432]\n",
            " [1501 1506  818]]\n",
            "Epoch 25: Train Loss: 1.0772 | Train Acc: 38.95%   Val Loss:   1.0994 | Val Acc:   33.47%\n",
            "Validation Confusion Matrix:\n",
            "[[1030 2371  863]\n",
            " [ 427 1670  431]\n",
            " [ 861 2111  853]]\n",
            "Epoch 26: Train Loss: 1.0762 | Train Acc: 38.69%   Val Loss:   1.0952 | Val Acc:   36.94%\n",
            "Validation Confusion Matrix:\n",
            "[[1850 1470  944]\n",
            " [ 882 1156  490]\n",
            " [1580 1329  916]]\n",
            "Epoch 27: Train Loss: 1.0751 | Train Acc: 38.93%   Val Loss:   1.0980 | Val Acc:   35.44%\n",
            "Validation Confusion Matrix:\n",
            "[[1117 1733 1414]\n",
            " [ 515 1283  730]\n",
            " [ 897 1565 1363]]\n",
            "Epoch 28: Train Loss: 1.0735 | Train Acc: 39.12%   Val Loss:   1.1009 | Val Acc:   33.92%\n",
            "Validation Confusion Matrix:\n",
            "[[1113 2201  950]\n",
            " [ 479 1568  481]\n",
            " [ 901 2004  920]]\n",
            "Epoch 29: Train Loss: 1.0732 | Train Acc: 39.50%   Val Loss:   1.0979 | Val Acc:   33.85%\n",
            "Validation Confusion Matrix:\n",
            "[[1182 2249  833]\n",
            " [ 533 1592  403]\n",
            " [1021 1984  820]]\n",
            "Epoch 30: Train Loss: 1.0721 | Train Acc: 39.70%   Val Loss:   1.0986 | Val Acc:   35.80%\n",
            "Validation Confusion Matrix:\n",
            "[[1572 1685 1007]\n",
            " [ 743 1277  508]\n",
            " [1348 1525  952]]\n",
            "Epoch 31: Train Loss: 1.0709 | Train Acc: 39.54%   Val Loss:   1.1020 | Val Acc:   36.62%\n",
            "Validation Confusion Matrix:\n",
            "[[1654 1448 1162]\n",
            " [ 798 1134  596]\n",
            " [1392 1333 1100]]\n",
            "Epoch 32: Train Loss: 1.0701 | Train Acc: 39.84%   Val Loss:   1.1043 | Val Acc:   34.30%\n",
            "Validation Confusion Matrix:\n",
            "[[1056 2050 1158]\n",
            " [ 490 1455  583]\n",
            " [ 874 1820 1131]]\n",
            "Epoch 33: Train Loss: 1.0684 | Train Acc: 40.00%   Val Loss:   1.1036 | Val Acc:   34.60%\n",
            "Validation Confusion Matrix:\n",
            "[[1548 2013  703]\n",
            " [ 697 1453  378]\n",
            " [1337 1816  672]]\n",
            "Epoch 34: Train Loss: 1.0677 | Train Acc: 40.10%   Val Loss:   1.1035 | Val Acc:   34.82%\n",
            "Validation Confusion Matrix:\n",
            "[[1721 1993  550]\n",
            " [ 825 1424  279]\n",
            " [1458 1815  552]]\n",
            "Epoch 35: Train Loss: 1.0659 | Train Acc: 40.49%   Val Loss:   1.1022 | Val Acc:   35.71%\n",
            "Validation Confusion Matrix:\n",
            "[[1528 1636 1100]\n",
            " [ 717 1224  587]\n",
            " [1294 1492 1039]]\n",
            "Epoch 36: Train Loss: 1.0638 | Train Acc: 40.76%   Val Loss:   1.1061 | Val Acc:   34.60%\n",
            "Validation Confusion Matrix:\n",
            "[[1660 1874  730]\n",
            " [ 743 1339  446]\n",
            " [1399 1752  674]]\n",
            "Epoch 37: Train Loss: 1.0626 | Train Acc: 40.82%   Val Loss:   1.1041 | Val Acc:   34.90%\n",
            "Validation Confusion Matrix:\n",
            "[[1425 1749 1090]\n",
            " [ 678 1300  550]\n",
            " [1230 1615  980]]\n",
            "Epoch 38: Train Loss: 1.0614 | Train Acc: 41.28%   Val Loss:   1.1121 | Val Acc:   34.26%\n",
            "Validation Confusion Matrix:\n",
            "[[1055 1865 1344]\n",
            " [ 472 1349  707]\n",
            " [ 875 1717 1233]]\n",
            "Epoch 39: Train Loss: 1.0595 | Train Acc: 41.22%   Val Loss:   1.1102 | Val Acc:   35.98%\n",
            "Validation Confusion Matrix:\n",
            "[[1390 1226 1648]\n",
            " [ 720  955  853]\n",
            " [1205 1145 1475]]\n",
            "Epoch 40: Train Loss: 1.0588 | Train Acc: 41.57%   Val Loss:   1.1146 | Val Acc:   35.79%\n",
            "Validation Confusion Matrix:\n",
            "[[1419 1552 1293]\n",
            " [ 675 1161  692]\n",
            " [1196 1409 1220]]\n",
            "Epoch 41: Train Loss: 1.0569 | Train Acc: 41.65%   Val Loss:   1.1083 | Val Acc:   36.02%\n",
            "Validation Confusion Matrix:\n",
            "[[1957 1524  783]\n",
            " [ 971 1120  437]\n",
            " [1711 1367  747]]\n",
            "Epoch 42: Train Loss: 1.0548 | Train Acc: 41.80%   Val Loss:   1.1118 | Val Acc:   34.91%\n",
            "Validation Confusion Matrix:\n",
            "[[1325 1807 1132]\n",
            " [ 651 1317  560]\n",
            " [1146 1615 1064]]\n",
            "Epoch 43: Train Loss: 1.0529 | Train Acc: 42.05%   Val Loss:   1.1110 | Val Acc:   36.12%\n",
            "Validation Confusion Matrix:\n",
            "[[1726 1538 1000]\n",
            " [ 834 1178  516]\n",
            " [1472 1422  931]]\n",
            "Epoch 44: Train Loss: 1.0525 | Train Acc: 42.34%   Val Loss:   1.1106 | Val Acc:   36.27%\n",
            "Validation Confusion Matrix:\n",
            "[[1758 1519  987]\n",
            " [ 854 1148  526]\n",
            " [1494 1386  945]]\n",
            "Epoch 45: Train Loss: 1.0502 | Train Acc: 42.20%   Val Loss:   1.1152 | Val Acc:   35.84%\n",
            "Validation Confusion Matrix:\n",
            "[[1626 1595 1043]\n",
            " [ 778 1206  544]\n",
            " [1403 1449  973]]\n",
            "Epoch 46: Train Loss: 1.0481 | Train Acc: 42.64%   Val Loss:   1.1188 | Val Acc:   34.75%\n",
            "Validation Confusion Matrix:\n",
            "[[1380 1660 1224]\n",
            " [ 653 1244  631]\n",
            " [1195 1565 1065]]\n",
            "Epoch 47: Train Loss: 1.0472 | Train Acc: 42.67%   Val Loss:   1.1262 | Val Acc:   32.99%\n",
            "Validation Confusion Matrix:\n",
            "[[1083 2271  910]\n",
            " [ 489 1573  466]\n",
            " [ 952 2026  847]]\n",
            "Epoch 48: Train Loss: 1.0447 | Train Acc: 42.94%   Val Loss:   1.1288 | Val Acc:   35.04%\n",
            "Validation Confusion Matrix:\n",
            "[[1381 1478 1405]\n",
            " [ 705 1114  709]\n",
            " [1217 1383 1225]]\n",
            "Epoch 49: Train Loss: 1.0434 | Train Acc: 43.16%   Val Loss:   1.1239 | Val Acc:   34.49%\n",
            "Validation Confusion Matrix:\n",
            "[[1007 1487 1770]\n",
            " [ 500 1137  891]\n",
            " [ 926 1381 1518]]\n",
            "Epoch 50: Train Loss: 1.0419 | Train Acc: 43.20%   Val Loss:   1.1293 | Val Acc:   33.86%\n",
            "Validation Confusion Matrix:\n",
            "[[1184 1993 1087]\n",
            " [ 553 1429  546]\n",
            " [1034 1809  982]]\n",
            "Epoch 51: Train Loss: 1.0396 | Train Acc: 43.51%   Val Loss:   1.1234 | Val Acc:   35.35%\n",
            "Validation Confusion Matrix:\n",
            "[[1386 1672 1206]\n",
            " [ 675 1257  596]\n",
            " [1202 1513 1110]]\n",
            "Epoch 52: Train Loss: 1.0368 | Train Acc: 44.01%   Val Loss:   1.1278 | Val Acc:   35.34%\n",
            "Validation Confusion Matrix:\n",
            "[[1350 1686 1228]\n",
            " [ 660 1263  605]\n",
            " [1155 1531 1139]]\n",
            "Epoch 53: Train Loss: 1.0360 | Train Acc: 44.02%   Val Loss:   1.1331 | Val Acc:   35.61%\n",
            "Validation Confusion Matrix:\n",
            "[[1343 1326 1595]\n",
            " [ 658 1038  832]\n",
            " [1159 1266 1400]]\n",
            "Epoch 54: Train Loss: 1.0339 | Train Acc: 44.05%   Val Loss:   1.1389 | Val Acc:   35.15%\n",
            "Validation Confusion Matrix:\n",
            "[[1334 1468 1462]\n",
            " [ 675 1090  763]\n",
            " [1145 1372 1308]]\n",
            "Epoch 55: Train Loss: 1.0314 | Train Acc: 44.36%   Val Loss:   1.1347 | Val Acc:   34.55%\n",
            "Validation Confusion Matrix:\n",
            "[[1304 1719 1241]\n",
            " [ 656 1243  629]\n",
            " [1115 1589 1121]]\n",
            "Epoch 56: Train Loss: 1.0308 | Train Acc: 44.44%   Val Loss:   1.1445 | Val Acc:   33.35%\n",
            "Validation Confusion Matrix:\n",
            "[[1054 2073 1137]\n",
            " [ 505 1457  566]\n",
            " [ 910 1885 1030]]\n",
            "Epoch 57: Train Loss: 1.0279 | Train Acc: 44.60%   Val Loss:   1.1409 | Val Acc:   36.12%\n",
            "Validation Confusion Matrix:\n",
            "[[1494 1294 1476]\n",
            " [ 737 1021  770]\n",
            " [1291 1214 1320]]\n",
            "Epoch 58: Train Loss: 1.0268 | Train Acc: 44.76%   Val Loss:   1.1441 | Val Acc:   34.93%\n",
            "Validation Confusion Matrix:\n",
            "[[1241 1414 1609]\n",
            " [ 647 1068  813]\n",
            " [1088 1338 1399]]\n",
            "Epoch 59: Train Loss: 1.0241 | Train Acc: 45.05%   Val Loss:   1.1469 | Val Acc:   35.05%\n",
            "Validation Confusion Matrix:\n",
            "[[1263 1573 1428]\n",
            " [ 624 1181  723]\n",
            " [1099 1449 1277]]\n",
            "Epoch 60: Train Loss: 1.0227 | Train Acc: 45.12%   Val Loss:   1.1456 | Val Acc:   35.14%\n",
            "Validation Confusion Matrix:\n",
            "[[1265 1452 1547]\n",
            " [ 661 1105  762]\n",
            " [1122 1342 1361]]\n",
            "Epoch 61: Train Loss: 1.0187 | Train Acc: 45.50%   Val Loss:   1.1573 | Val Acc:   32.81%\n",
            "Validation Confusion Matrix:\n",
            "[[1054 2049 1161]\n",
            " [ 504 1407  617]\n",
            " [ 905 1898 1022]]\n",
            "Epoch 62: Train Loss: 1.0165 | Train Acc: 45.77%   Val Loss:   1.1523 | Val Acc:   35.22%\n",
            "Validation Confusion Matrix:\n",
            "[[1536 1534 1194]\n",
            " [ 785 1160  583]\n",
            " [1344 1438 1043]]\n",
            "Epoch 63: Train Loss: 1.0144 | Train Acc: 45.78%   Val Loss:   1.1512 | Val Acc:   34.90%\n",
            "Validation Confusion Matrix:\n",
            "[[1440 1573 1251]\n",
            " [ 722 1175  631]\n",
            " [1277 1458 1090]]\n",
            "Epoch 64: Train Loss: 1.0138 | Train Acc: 45.81%   Val Loss:   1.1553 | Val Acc:   35.82%\n",
            "Validation Confusion Matrix:\n",
            "[[1657 1434 1173]\n",
            " [ 813 1098  617]\n",
            " [1424 1353 1048]]\n",
            "Epoch 65: Train Loss: 1.0106 | Train Acc: 46.36%   Val Loss:   1.1535 | Val Acc:   34.44%\n",
            "Validation Confusion Matrix:\n",
            "[[1358 1606 1300]\n",
            " [ 704 1171  653]\n",
            " [1250 1447 1128]]\n",
            "Epoch 66: Train Loss: 1.0090 | Train Acc: 46.51%   Val Loss:   1.1679 | Val Acc:   35.76%\n",
            "Validation Confusion Matrix:\n",
            "[[1672 1481 1111]\n",
            " [ 829 1115  584]\n",
            " [1422 1393 1010]]\n",
            "Epoch 67: Train Loss: 1.0069 | Train Acc: 46.55%   Val Loss:   1.1618 | Val Acc:   34.40%\n",
            "Validation Confusion Matrix:\n",
            "[[1155 1428 1681]\n",
            " [ 589 1063  876]\n",
            " [1029 1362 1434]]\n",
            "Epoch 68: Train Loss: 1.0031 | Train Acc: 46.87%   Val Loss:   1.1782 | Val Acc:   34.63%\n",
            "Validation Confusion Matrix:\n",
            "[[1279 1615 1370]\n",
            " [ 647 1202  679]\n",
            " [1134 1495 1196]]\n",
            "Epoch 69: Train Loss: 1.0011 | Train Acc: 47.21%   Val Loss:   1.1727 | Val Acc:   35.34%\n",
            "Validation Confusion Matrix:\n",
            "[[1454 1462 1348]\n",
            " [ 728 1104  696]\n",
            " [1247 1384 1194]]\n",
            "Epoch 70: Train Loss: 0.9983 | Train Acc: 47.22%   Val Loss:   1.1669 | Val Acc:   35.93%\n",
            "Validation Confusion Matrix:\n",
            "[[1693 1362 1209]\n",
            " [ 874 1014  640]\n",
            " [1460 1257 1108]]\n",
            "Epoch 71: Train Loss: 0.9987 | Train Acc: 47.39%   Val Loss:   1.1830 | Val Acc:   35.27%\n",
            "Validation Confusion Matrix:\n",
            "[[1456 1456 1352]\n",
            " [ 738 1069  721]\n",
            " [1239 1366 1220]]\n",
            "Epoch 72: Train Loss: 0.9937 | Train Acc: 47.56%   Val Loss:   1.1836 | Val Acc:   35.88%\n",
            "Validation Confusion Matrix:\n",
            "[[1580 1185 1499]\n",
            " [ 838  922  768]\n",
            " [1362 1156 1307]]\n",
            "Epoch 73: Train Loss: 0.9930 | Train Acc: 47.91%   Val Loss:   1.1826 | Val Acc:   34.07%\n",
            "Validation Confusion Matrix:\n",
            "[[1142 1688 1434]\n",
            " [ 566 1216  746]\n",
            " [ 982 1584 1259]]\n",
            "Epoch 74: Train Loss: 0.9886 | Train Acc: 47.84%   Val Loss:   1.1876 | Val Acc:   34.82%\n",
            "Validation Confusion Matrix:\n",
            "[[1490 1570 1204]\n",
            " [ 769 1148  611]\n",
            " [1296 1470 1059]]\n",
            "Epoch 75: Train Loss: 0.9907 | Train Acc: 47.97%   Val Loss:   1.1809 | Val Acc:   35.48%\n",
            "Validation Confusion Matrix:\n",
            "[[1500 1364 1400]\n",
            " [ 761 1040  727]\n",
            " [1289 1309 1227]]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m logits = model(stk, emo) \u001b[38;5;66;03m# åªä¼ å…¥ stock å’Œ emotion\u001b[39;00m\n\u001b[32m     66\u001b[39m loss = criterion(logits, y)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\u001b[39;00m\n\u001b[32m     70\u001b[39m optimizer.step()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/py312/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/py312/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/py312/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# ======= 1ï¸âƒ£ è¶…å‚æ•°é…ç½® =======\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "num_classes = 3\n",
        "lr = 3e-5             # å»ºè®®ä½¿ç”¨ç¨ä½çš„å­¦ä¹ ç‡\n",
        "num_epochs = 100\n",
        "save_path = \"best_model_2_modal.pt\"\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "print(\"âš–ï¸ è®¡ç®—ç±»åˆ«æƒé‡...\")\n",
        "# 1. ç»Ÿè®¡è®­ç»ƒé›†ä¸­æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°\n",
        "train_labels = [s['label'].item() for s in train_samples]\n",
        "label_counts = Counter(sorted(train_labels))\n",
        "\n",
        "# 2. è®¡ç®—æƒé‡ (ä½¿ç”¨æ ‡å‡†çš„åæ¯”é¢‘ç‡å…¬å¼)\n",
        "#    å…¬å¼: weight[c] = total_samples / (num_classes * count[c])\n",
        "total_samples = len(train_labels)\n",
        "weights = [total_samples / (len(label_counts) * label_counts[i]) for i in sorted(label_counts.keys())]\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "\n",
        "print(f\"ç±»åˆ«è®¡æ•°: {label_counts}\")\n",
        "print(f\"è®¡ç®—å‡ºçš„ç±»åˆ«æƒé‡: {class_weights}\")\n",
        "\n",
        "# ======= 2ï¸âƒ£ æ¨¡å‹å®ä¾‹åŒ– =======\n",
        "# ä½¿ç”¨æ–°çš„ TwoModalTransformer å¹¶ä¼ å…¥å»ºè®®çš„ã€æ›´æŠ—è¿‡æ‹Ÿåˆçš„è¶…å‚æ•°\n",
        "model = TwoModalTransformer(\n",
        "    stock_dim=len(stock_cols),\n",
        "    emo_dim=len(emotion_cols),\n",
        "    proj_dim=64,\n",
        "    hidden_dim=256,\n",
        "    num_heads=4,\n",
        "    num_layers=2,\n",
        "    num_classes=num_classes,\n",
        "    max_len=window_size + 1, # è®¾ç½®ä¸ºç•¥å¤§äºçª—å£é•¿åº¦\n",
        "    dropout=0.1              # ä½¿ç”¨æ›´å¼ºçš„ dropout\n",
        ").to(device)\n",
        "\n",
        "# ======= 3ï¸âƒ£ ä¼˜åŒ–å™¨ + æŸå¤±å‡½æ•° =======\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4) # ä½¿ç”¨æ›´å¼ºçš„ weight_decay\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "# ======= 4ï¸âƒ£ è®­ç»ƒä¸éªŒè¯å¾ªç¯ =======\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        emo = batch[\"emotion\"].to(device)\n",
        "        stk = batch[\"stock\"].to(device)\n",
        "        y = batch[\"label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(stk, emo) # åªä¼ å…¥ stock å’Œ emotion\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * y.size(0)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "    train_loss /= total\n",
        "    # scheduler.step()\n",
        "\n",
        "    # === Validate ===\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            emo = batch[\"emotion\"].to(device)\n",
        "            stk = batch[\"stock\"].to(device)\n",
        "            y = batch[\"label\"].to(device)\n",
        "\n",
        "            logits = model(stk, emo)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "            val_loss += loss.item() * y.size(0)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            val_correct += (pred == y).sum().item()\n",
        "            val_total += y.size(0)\n",
        "\n",
        "            all_val_preds.extend(pred.cpu().numpy())\n",
        "            all_val_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    val_loss /= val_total\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\", end = \"   \")\n",
        "    print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc*100:.2f}%\")\n",
        "\n",
        "    cm = confusion_matrix(all_val_labels, all_val_preds)\n",
        "    print(\"Validation Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"âœ… Best model saved with Val Acc: {best_val_acc:.4f}\")\n",
        "\n",
        "print(f\"\\nğŸ Training Finished! Best Val Acc = {best_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "511ecf45",
      "metadata": {
        "id": "511ecf45"
      },
      "source": [
        "# 1 day, lgbm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9baf3009",
      "metadata": {
        "id": "9baf3009"
      },
      "source": [
        "## direct run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c39745a",
      "metadata": {
        "id": "7c39745a",
        "outputId": "1f649c97-474d-44dc-c8b2-fa070c795d03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "å¼€å§‹è®­ç»ƒæ¨¡å‹...\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[515]\tvalid_0's multi_logloss: 1.05917\n",
            "\n",
            "åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°...\n",
            "Accuracy: 0.4215\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.80      0.55      8865\n",
            "           1       0.44      0.17      0.25      5399\n",
            "           2       0.41      0.16      0.23      7743\n",
            "\n",
            "    accuracy                           0.42     22007\n",
            "   macro avg       0.42      0.38      0.34     22007\n",
            "weighted avg       0.42      0.42      0.37     22007\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# å‡è®¾æ‚¨çš„æ•°æ®åœ¨ä¸€ä¸ªåä¸º all_in_one_file çš„ DataFrame ä¸­\n",
        "all_in_one_file = pd.read_parquet('merged.parquet')\n",
        "\n",
        "# é€‰æ‹©æ‚¨æŒ‡å®šçš„ç‰¹å¾åˆ—\n",
        "emotion_cols = [\n",
        "    'Emotion_Index_closing',\n",
        "    'Emotion_Index_trading',\n",
        "    'Emotion_Momentum_3d_closing',\n",
        "    'Emotion_Momentum_3d_trading'\n",
        "]\n",
        "stock_cols = ['log_return', 'amplitude']\n",
        "\n",
        "feature_cols = emotion_cols + stock_cols\n",
        "# feature_cols = list(set(all_in_one_file.columns.tolist()) - set(['Label', 'Date', 'StockCode']))\n",
        "target_col = 'Label'\n",
        "\n",
        "# ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—å’Œç›®æ ‡åˆ—éƒ½åœ¨ DataFrame ä¸­\n",
        "required_cols = feature_cols + [target_col]\n",
        "if not all(col in all_in_one_file.columns for col in required_cols):\n",
        "    raise ValueError(\"DataFrame ä¸­ç¼ºå°‘æ‰€éœ€çš„åˆ—\")\n",
        "\n",
        "# å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾\n",
        "X = all_in_one_file[feature_cols]\n",
        "y = all_in_one_file[target_col]\n",
        "\n",
        "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
        "# ä¸ºäº†ä¿æŒæ—¶é—´åºåˆ—çš„é¡ºåºï¼Œæˆ‘ä»¬ä¸éšæœºæ‰“ä¹±æ•°æ®\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, shuffle=False\n",
        ")\n",
        "\n",
        "# --- å‚æ•°è®¾ç½® ---\n",
        "# å®šä¹‰ LightGBM çš„å‚æ•°å­—å…¸\n",
        "# æ‚¨å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¿™äº›å€¼\n",
        "params = {\n",
        "    'objective': 'multiclass',          # ç›®æ ‡å‡½æ•°ï¼šäºŒåˆ†ç±»\n",
        "    'num_class': 3,\n",
        "    'metric': 'multi_logloss',     # è¯„ä¼°æŒ‡æ ‡\n",
        "    'boosting_type': 'gbdt',        # æå‡ç±»å‹\n",
        "    'n_estimators': 2000,           # æ ‘çš„æœ€å¤§æ•°é‡ï¼ˆä¸€ä¸ªè¾ƒå¤§çš„å€¼ï¼Œé…åˆæ—©åœä½¿ç”¨ï¼‰\n",
        "    'learning_rate': 0.01,          # å­¦ä¹ ç‡\n",
        "    'num_leaves': 31,               # æ¯æ£µæ ‘çš„å¶å­èŠ‚ç‚¹æ•°\n",
        "    'max_depth': -1,                # æ ‘çš„æœ€å¤§æ·±åº¦, -1 è¡¨ç¤ºæ— é™åˆ¶\n",
        "    'seed': 42,                     # éšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°\n",
        "    'n_jobs': -1,                   # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ CPU æ ¸å¿ƒ\n",
        "    'verbose': 0,                  # æŠ‘åˆ¶è¾“å‡ºä¿¡æ¯\n",
        "    'colsample_bytree': 0.8,        # æ„å»ºæ¯æ£µæ ‘æ—¶éšæœºé€‰æ‹©çš„ç‰¹å¾æ¯”ä¾‹\n",
        "    'subsample': 0.8,               # è®­ç»ƒæ¯æ£µæ ‘æ—¶ä½¿ç”¨çš„æ•°æ®ï¼ˆè¡Œï¼‰æ¯”ä¾‹\n",
        "    'reg_alpha': 0.1,               # L1 æ­£åˆ™åŒ–\n",
        "    'reg_lambda': 0.1,              # L2 æ­£åˆ™åŒ–\n",
        "    'device': 'gpu'\n",
        "}\n",
        "best_params = {'learning_rate': 0.009054971028569354,\n",
        " 'num_leaves': 25,\n",
        " 'subsample': 0.7545176635949,\n",
        " 'colsample_bytree': 0.6331222368386825,\n",
        " 'reg_alpha': 8.847018473975617e-06,\n",
        " 'reg_lambda': 1.990125011362269,\n",
        " 'objective': 'multiclass',\n",
        " 'num_class': 3,\n",
        " 'metric': 'multi_logloss',\n",
        " 'n_estimators': 2000,\n",
        " 'seed': 42,\n",
        " 'n_jobs': -1,\n",
        " 'boosting_type': 'gbdt'}\n",
        "params.update(best_params)\n",
        "\n",
        "# ä½¿ç”¨ **params å°†å­—å…¸è§£åŒ…æˆå…³é”®å­—å‚æ•°æ¥åˆå§‹åŒ–åˆ†ç±»å™¨\n",
        "lgbm = lgb.LGBMClassifier(**params)\n",
        "\n",
        "# --- æ¨¡å‹è®­ç»ƒä¸æ—©åœ ---\n",
        "print(\"å¼€å§‹è®­ç»ƒæ¨¡å‹...\")\n",
        "lgbm.fit(X_train, y_train,\n",
        "         eval_set=[(X_test, y_test)],        # è®¾ç½®éªŒè¯é›†ç”¨äºæ—©åœ\n",
        "         eval_metric='multi_logloss',               # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°çš„æŒ‡æ ‡\n",
        "         callbacks=[lgb.early_stopping(200, verbose=True)]) # å¦‚æœéªŒè¯é›†æŒ‡æ ‡åœ¨ 100 è½®å†…æ²¡æœ‰æ”¹å–„ï¼Œåˆ™åœæ­¢è®­ç»ƒ\n",
        "\n",
        "# --- æ¨¡å‹è¯„ä¼° ---\n",
        "print(\"\\nåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°...\")\n",
        "y_pred = lgbm.predict(X_test)\n",
        "y_pred_proba = lgbm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# --- ç‰¹å¾é‡è¦æ€§ ---\n",
        "# print(\"\\nç»˜åˆ¶ç‰¹å¾é‡è¦æ€§...\")\n",
        "# lgb.plot_importance(lgbm, max_num_features=len(feature_cols), figsize=(10, 6), importance_type='gain')\n",
        "# plt.title('Feature Importance (Gain)')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc241f1",
      "metadata": {
        "id": "ecc241f1",
        "outputId": "db24776a-9cde-4cfa-fff0-c73874715f47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<lightgbm.basic.Booster at 0x15550f69e780>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lgbm.booster_.save_model('lgbm_42.6.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90add199",
      "metadata": {
        "id": "90add199"
      },
      "source": [
        "## optuna tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75cc3999",
      "metadata": {
        "id": "75cc3999"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import optuna\n",
        "\n",
        "# å‡è®¾æ‚¨çš„æ•°æ®åœ¨ä¸€ä¸ªåä¸º all_in_one_file çš„ DataFrame ä¸­\n",
        "# å¹¶ä¸” 'label' åˆ—åŒ…å« 0, 1, 2 ä¸‰ä¸ªç±»åˆ«\n",
        "all_in_one_file = pd.read_parquet('merged.parquet')\n",
        "\n",
        "# --- 1. æ•°æ®å‡†å¤‡ ---\n",
        "emotion_cols = [\n",
        "    'Emotion_Index_closing',\n",
        "    'Emotion_Index_trading',\n",
        "    'Emotion_Momentum_3d_closing',\n",
        "    'Emotion_Momentum_3d_trading'\n",
        "]\n",
        "stock_cols = ['log_return', 'amplitude']\n",
        "feature_cols = emotion_cols + stock_cols\n",
        "target_col = 'Label'\n",
        "\n",
        "X = all_in_one_file[feature_cols]\n",
        "y = all_in_one_file[target_col]\n",
        "\n",
        "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† (hold-out set)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, shuffle=False\n",
        ")\n",
        "\n",
        "# --- 2. Optuna ç›®æ ‡å‡½æ•° ---\n",
        "def objective(trial):\n",
        "    # ä¸ºè¶…å‚æ•°å®šä¹‰æœç´¢ç©ºé—´\n",
        "    params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': 3,\n",
        "        'metric': 'multi_logloss',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'n_estimators': 2000,\n",
        "        'seed': 42,\n",
        "        'n_jobs': -1,\n",
        "        'verbose': -1,\n",
        "        'device': 'gpu',\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': -1,\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
        "    }\n",
        "\n",
        "    # å®ä¾‹åŒ–æ¨¡å‹\n",
        "    model = lgb.LGBMClassifier(**params)\n",
        "\n",
        "    # è®­ç»ƒæ¨¡å‹\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_test, y_test)],\n",
        "              eval_metric='multi_logloss',\n",
        "              callbacks=[lgb.early_stopping(100, verbose=False)])\n",
        "\n",
        "    # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°å¹¶è¿”å›å‡†ç¡®ç‡\n",
        "    preds = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, preds)\n",
        "    return accuracy\n",
        "\n",
        "# --- 3. åˆ›å»ºå¹¶è¿è¡Œ Optuna Study ---\n",
        "# æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–å‡†ç¡®ç‡ï¼Œæ‰€ä»¥ direction='maximize'\n",
        "study = optuna.create_study(direction='maximize')\n",
        "print(\"å¼€å§‹ä½¿ç”¨ Optuna è¿›è¡Œ 100 æ¬¡å‚æ•°è°ƒä¼˜...\")\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# --- 4. æ‰“å°æœ€ä½³ç»“æœ ---\n",
        "print(\"\\nè°ƒä¼˜å®Œæˆï¼\")\n",
        "print(\"æœ€ä½³è¯•éªŒ:\")\n",
        "best_trial = study.best_trial\n",
        "print(f\"  ä»·å€¼ (Accuracy): {best_trial.value:.4f}\")\n",
        "print(\"  æœ€ä½³å‚æ•°: \")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "# --- 5. ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹ ---\n",
        "print(\"\\nä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹...\")\n",
        "best_params = best_trial.params\n",
        "# æ·»åŠ ä¸€äº›åœ¨è°ƒä¼˜ä¸­æœªè®¾ç½®çš„å›ºå®šå‚æ•°\n",
        "best_params['objective'] = 'multiclass'\n",
        "best_params['num_class'] = 3\n",
        "best_params['metric'] = 'multi_logloss'\n",
        "best_params['n_estimators'] = 2000 # å¯ä»¥ç”¨ä¸€ä¸ªè¾ƒå¤§çš„å€¼ï¼Œé…åˆæ—©åœ\n",
        "best_params['seed'] = 42\n",
        "best_params['n_jobs'] = -1\n",
        "best_params['boosting_type'] = 'gbdt'\n",
        "\n",
        "\n",
        "final_model = lgb.LGBMClassifier(**best_params)\n",
        "\n",
        "# åœ¨å®Œæ•´çš„è®­ç»ƒé›†ä¸Šè®­ç»ƒï¼Œä½¿ç”¨æµ‹è¯•é›†ä½œä¸ºæ—©åœçš„éªŒè¯é›†\n",
        "final_model.fit(X_train, y_train,\n",
        "                eval_set=[(X_test, y_test)],\n",
        "                eval_metric='multi_logloss',\n",
        "                callbacks=[lgb.early_stopping(100, verbose=True)])\n",
        "\n",
        "# --- 6. è¯„ä¼°æœ€ç»ˆæ¨¡å‹ ---\n",
        "print(\"\\nåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹...\")\n",
        "y_pred_final = final_model.predict(X_test)\n",
        "\n",
        "final_accuracy = accuracy_score(y_test, y_pred_final)\n",
        "print(f\"\\nFinal Test Set Accuracy: {final_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nFinal Model Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_final))\n",
        "\n",
        "# ç»˜åˆ¶æœ€ç»ˆæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§\n",
        "lgb.plot_importance(final_model, max_num_features=len(feature_cols), figsize=(10, 6), importance_type='gain')\n",
        "plt.title('Final Model Feature Importance (Gain)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fefe2ab",
      "metadata": {
        "id": "6fefe2ab",
        "outputId": "786dd0cd-f01d-4bf8-fbf9-fea83f91c40d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'learning_rate': 0.009054971028569354,\n",
              " 'num_leaves': 25,\n",
              " 'subsample': 0.7545176635949,\n",
              " 'colsample_bytree': 0.6331222368386825,\n",
              " 'reg_alpha': 8.847018473975617e-06,\n",
              " 'reg_lambda': 1.990125011362269,\n",
              " 'objective': 'multiclass',\n",
              " 'num_class': 3,\n",
              " 'metric': 'multi_logloss',\n",
              " 'n_estimators': 2000,\n",
              " 'seed': 42,\n",
              " 'n_jobs': -1,\n",
              " 'boosting_type': 'gbdt'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_trial.params"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46758244",
      "metadata": {
        "id": "46758244"
      },
      "source": [
        "## Finetune on specific stock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a6a197",
      "metadata": {
        "id": "f8a6a197",
        "outputId": "095ed591-410e-46ae-fbe1-0c85e97c7524"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [01:28<00:00,  2.61it/s]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===== 1. åŠ è½½æ•°æ®ä¸æ¨¡å‹ =====\n",
        "# åŠ è½½å’Œä¹‹å‰è®­ç»ƒæ—¶ç›¸åŒçš„æ•°æ®\n",
        "all_in_one_file = pd.read_parquet('merged.parquet')\n",
        "# åŠ è½½ä½ ä¿å­˜çš„é€šç”¨LGBMæ¨¡å‹\n",
        "general_model_path = 'lgbm_42.6.txt'\n",
        "\n",
        "# å®šä¹‰å’Œä¹‹å‰è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„ç‰¹å¾\n",
        "emotion_cols = [\n",
        "    'Emotion_Index_closing',\n",
        "    'Emotion_Index_trading',\n",
        "    'Emotion_Momentum_3d_closing',\n",
        "    'Emotion_Momentum_3d_trading'\n",
        "]\n",
        "stock_cols = ['log_return', 'amplitude']\n",
        "feature_cols = emotion_cols + stock_cols\n",
        "target_col = 'Label'\n",
        "\n",
        "# ===== 2. ç­›é€‰ç‰¹å®šè‚¡ç¥¨æ•°æ® =====\n",
        "stock_accs = {}\n",
        "for stock_code_to_finetune in tqdm(all_in_one_file['StockCode'].unique()):\n",
        "    df_stock = all_in_one_file[all_in_one_file['StockCode'] == stock_code_to_finetune].copy()\n",
        "    df_stock = df_stock.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    if df_stock.empty:\n",
        "        print(f\"âŒ è­¦å‘Š: æœªæ‰¾åˆ°è‚¡ç¥¨ä»£ç  {stock_code_to_finetune} çš„æ•°æ®ã€‚\")\n",
        "    else:\n",
        "        # ===== 3. å‡†å¤‡å¾®è°ƒæ•°æ® =====\n",
        "        X_stock = df_stock[feature_cols]\n",
        "        y_stock = df_stock[target_col]\n",
        "\n",
        "        # æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†å¾®è°ƒç”¨çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
        "        X_train_ft, X_test_ft, y_train_ft, y_test_ft = train_test_split(\n",
        "            X_stock, y_stock, test_size=0.2, shuffle=False\n",
        "        )\n",
        "        # print(f\"ğŸ“Š å¾®è°ƒæ•°æ®é›†: {len(X_train_ft)} è®­ç»ƒæ ·æœ¬, {len(X_test_ft)} æµ‹è¯•æ ·æœ¬ã€‚\")\n",
        "\n",
        "\n",
        "        # ===== 4. æ‰§è¡Œå¾®è°ƒ =====\n",
        "        # ä½¿ç”¨å’Œé€šç”¨æ¨¡å‹ç›¸åŒçš„å‚æ•°é…ç½®ï¼Œä½†å¯ä»¥é™ä½å­¦ä¹ ç‡ä»¥è¿›è¡Œæ›´ç²¾ç»†çš„è°ƒæ•´\n",
        "        # æˆ‘ä»¬è¿™é‡Œæ²¿ç”¨ä¹‹å‰çš„ best_params\n",
        "        best_params = {'learning_rate': 0.009054971028569354,\n",
        "         'num_leaves': 25,\n",
        "         'subsample': 0.7545176635949,\n",
        "         'colsample_bytree': 0.6331222368386825,\n",
        "         'reg_alpha': 8.847018473975617e-06,\n",
        "         'reg_lambda': 1.990125011362269,\n",
        "         'objective': 'multiclass',\n",
        "         'num_class': 3,\n",
        "         'metric': 'multi_logloss',\n",
        "         'n_estimators': 2000,\n",
        "         'seed': 42,\n",
        "         'verbose': -1,\n",
        "         'n_jobs': -1,\n",
        "         'boosting_type': 'gbdt',\n",
        "         'device': 'gpu'}\n",
        "\n",
        "        # é™ä½å­¦ä¹ ç‡æ˜¯ä¸€ä¸ªå¸¸è§çš„å¾®è°ƒæŠ€å·§ï¼Œæœ‰åŠ©äºæ¨¡å‹åœ¨å°æ•°æ®é›†ä¸Šæ›´å¥½åœ°æ”¶æ•›\n",
        "        finetune_params = best_params.copy()\n",
        "        finetune_params['learning_rate'] = 0.005 # ä¾‹å¦‚ï¼Œé™åˆ°æ›´ä½\n",
        "\n",
        "        lgbm_finetune = lgb.LGBMClassifier(**finetune_params)\n",
        "\n",
        "        # print(\"\\nğŸš€ å¼€å§‹å¾®è°ƒæ¨¡å‹...\")\n",
        "        lgbm_finetune.fit(\n",
        "            X_train_ft, y_train_ft,\n",
        "            eval_set=[(X_test_ft, y_test_ft)],\n",
        "            eval_metric='multi_logloss',\n",
        "            callbacks=[lgb.early_stopping(10, verbose=False)],\n",
        "            init_model=general_model_path  # <-- å¾®è°ƒçš„å…³é”®ï¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
        "        )\n",
        "\n",
        "        # ===== 5. è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹ =====\n",
        "        # print(\"\\nğŸ” è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹åœ¨ä¸“å±æµ‹è¯•é›†ä¸Šçš„è¡¨ç°...\")\n",
        "        y_pred_ft = lgbm_finetune.predict(X_test_ft)\n",
        "        accuracy_ft = accuracy_score(y_test_ft, y_pred_ft)\n",
        "        stock_accs[stock_code_to_finetune] = {\n",
        "            \"acc\": accuracy_ft,\n",
        "            \"v_counts\": y_test_ft.value_counts()\n",
        "        }\n",
        "\n",
        "        # print(f\"\\nFinetuned Model Accuracy: {accuracy_ft:.4f}\")\n",
        "        # print(\"\\nFinetuned Model Classification Report:\")\n",
        "        # print(classification_report(y_test_ft, y_pred_ft))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "929c7f9f",
      "metadata": {
        "id": "929c7f9f",
        "outputId": "1e0f1f8e-15b3-47d1-bad9-a22bb6912ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'000001': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  0    17\n",
              "  1    16\n",
              "  2    16\n",
              "  Name: count, dtype: int64},\n",
              " '000002': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    16\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '000063': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  2    21\n",
              "  0    20\n",
              "  1     8\n",
              "  Name: count, dtype: int64},\n",
              " '000301': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  2    22\n",
              "  0    21\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '000333': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  0    17\n",
              "  2    17\n",
              "  1    15\n",
              "  Name: count, dtype: int64},\n",
              " '000338': {'acc': 0.32653061224489793,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  2    18\n",
              "  1    13\n",
              "  Name: count, dtype: int64},\n",
              " '000568': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  1    14\n",
              "  2    12\n",
              "  Name: count, dtype: int64},\n",
              " '000596': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    18\n",
              "  1     8\n",
              "  Name: count, dtype: int64},\n",
              " '000630': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  2    15\n",
              "  1    15\n",
              "  Name: count, dtype: int64},\n",
              " '000651': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  1    19\n",
              "  0    17\n",
              "  2    13\n",
              "  Name: count, dtype: int64},\n",
              " '000661': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    24\n",
              "  2    16\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '000708': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  1    16\n",
              "  2    14\n",
              "  Name: count, dtype: int64},\n",
              " '000768': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    19\n",
              "  1     7\n",
              "  Name: count, dtype: int64},\n",
              " '000776': {'acc': 0.2857142857142857,\n",
              "  'v_counts': Label\n",
              "  1    19\n",
              "  2    16\n",
              "  0    14\n",
              "  Name: count, dtype: int64},\n",
              " '000786': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    17\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '000792': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    15\n",
              "  1    13\n",
              "  Name: count, dtype: int64},\n",
              " '000800': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  2    17\n",
              "  1    13\n",
              "  Name: count, dtype: int64},\n",
              " '000807': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    22\n",
              "  2    18\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '000876': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  0    24\n",
              "  2    17\n",
              "  1     8\n",
              "  Name: count, dtype: int64},\n",
              " '000895': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  2    17\n",
              "  0    16\n",
              "  1    16\n",
              "  Name: count, dtype: int64},\n",
              " '000938': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  2    23\n",
              "  0    15\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '000963': {'acc': 0.2653061224489796,\n",
              "  'v_counts': Label\n",
              "  2    17\n",
              "  0    17\n",
              "  1    15\n",
              "  Name: count, dtype: int64},\n",
              " '000977': {'acc': 0.5306122448979592,\n",
              "  'v_counts': Label\n",
              "  2    22\n",
              "  0    17\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '000983': {'acc': 0.3469387755102041,\n",
              "  'v_counts': Label\n",
              "  1    23\n",
              "  0    14\n",
              "  2    12\n",
              "  Name: count, dtype: int64},\n",
              " '000999': {'acc': 0.4897959183673469,\n",
              "  'v_counts': Label\n",
              "  0    22\n",
              "  1    14\n",
              "  2    13\n",
              "  Name: count, dtype: int64},\n",
              " '001289': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    25\n",
              "  1    13\n",
              "  2    11\n",
              "  Name: count, dtype: int64},\n",
              " '001979': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    17\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '002001': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  1    16\n",
              "  2    14\n",
              "  Name: count, dtype: int64},\n",
              " '002027': {'acc': 0.5306122448979592,\n",
              "  'v_counts': Label\n",
              "  0    22\n",
              "  2    18\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '002049': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  2    17\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '002050': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    24\n",
              "  2    19\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '002142': {'acc': 0.5306122448979592,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    16\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '002179': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    22\n",
              "  2    15\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '002180': {'acc': 0.5306122448979592,\n",
              "  'v_counts': Label\n",
              "  0    22\n",
              "  2    17\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '002230': {'acc': 0.5510204081632653,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    21\n",
              "  1     7\n",
              "  Name: count, dtype: int64},\n",
              " '002236': {'acc': 0.5306122448979592,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    19\n",
              "  1     7\n",
              "  Name: count, dtype: int64},\n",
              " '002241': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    21\n",
              "  1     5\n",
              "  Name: count, dtype: int64},\n",
              " '002252': {'acc': 0.3829787234042553,\n",
              "  'v_counts': Label\n",
              "  1    18\n",
              "  0    15\n",
              "  2    14\n",
              "  Name: count, dtype: int64},\n",
              " '002304': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  0    17\n",
              "  1    17\n",
              "  2    15\n",
              "  Name: count, dtype: int64},\n",
              " '002311': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  2    16\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '002352': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  2    17\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '002371': {'acc': 0.5306122448979592,\n",
              "  'v_counts': Label\n",
              "  0    22\n",
              "  2    19\n",
              "  1     8\n",
              "  Name: count, dtype: int64},\n",
              " '002422': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    15\n",
              "  1    13\n",
              "  Name: count, dtype: int64},\n",
              " '002460': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    22\n",
              "  2    18\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '002466': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  2    18\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '002475': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    25\n",
              "  2    18\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '002493': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  2    18\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '002600': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    20\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '002601': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    17\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '002648': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  2    18\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '002714': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  1    15\n",
              "  2    14\n",
              "  Name: count, dtype: int64},\n",
              " '002916': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  2    21\n",
              "  0    19\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '002920': {'acc': 0.6326530612244898,\n",
              "  'v_counts': Label\n",
              "  0    26\n",
              "  2    17\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '002938': {'acc': 0.4897959183673469,\n",
              "  'v_counts': Label\n",
              "  0    22\n",
              "  2    19\n",
              "  1     8\n",
              "  Name: count, dtype: int64},\n",
              " '003816': {'acc': 0.30612244897959184,\n",
              "  'v_counts': Label\n",
              "  0    17\n",
              "  1    17\n",
              "  2    15\n",
              "  Name: count, dtype: int64},\n",
              " '300014': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    19\n",
              "  1     7\n",
              "  Name: count, dtype: int64},\n",
              " '300015': {'acc': 0.5714285714285714,\n",
              "  'v_counts': Label\n",
              "  0    25\n",
              "  2    18\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '300033': {'acc': 0.4897959183673469,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    19\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '300059': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    20\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '300274': {'acc': 0.4897959183673469,\n",
              "  'v_counts': Label\n",
              "  0    26\n",
              "  2    16\n",
              "  1     7\n",
              "  Name: count, dtype: int64},\n",
              " '300308': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  0    24\n",
              "  2    22\n",
              "  1     3\n",
              "  Name: count, dtype: int64},\n",
              " '300316': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    19\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '300347': {'acc': 0.5510204081632653,\n",
              "  'v_counts': Label\n",
              "  0    27\n",
              "  2    15\n",
              "  1     7\n",
              "  Name: count, dtype: int64},\n",
              " '300394': {'acc': 0.5510204081632653,\n",
              "  'v_counts': Label\n",
              "  0    24\n",
              "  2    20\n",
              "  1     5\n",
              "  Name: count, dtype: int64},\n",
              " '300408': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  0    22\n",
              "  2    19\n",
              "  1     8\n",
              "  Name: count, dtype: int64},\n",
              " '300418': {'acc': 0.4897959183673469,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    22\n",
              "  1     4\n",
              "  Name: count, dtype: int64},\n",
              " '300433': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    24\n",
              "  2    16\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '300502': {'acc': 0.5510204081632653,\n",
              "  'v_counts': Label\n",
              "  0    22\n",
              "  2    21\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '300628': {'acc': 0.5306122448979592,\n",
              "  'v_counts': Label\n",
              "  2    22\n",
              "  0    21\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '300661': {'acc': 0.5306122448979592,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    18\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '300750': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    18\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '300759': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    25\n",
              "  2    19\n",
              "  1     5\n",
              "  Name: count, dtype: int64},\n",
              " '300760': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  2    17\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '300782': {'acc': 0.5918367346938775,\n",
              "  'v_counts': Label\n",
              "  0    27\n",
              "  2    17\n",
              "  1     5\n",
              "  Name: count, dtype: int64},\n",
              " '300832': {'acc': 0.5306122448979592,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    17\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '300999': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  0    24\n",
              "  2    16\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '301236': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    19\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '301269': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  0    22\n",
              "  2    19\n",
              "  1     8\n",
              "  Name: count, dtype: int64},\n",
              " '600000': {'acc': 0.32653061224489793,\n",
              "  'v_counts': Label\n",
              "  1    18\n",
              "  0    16\n",
              "  2    15\n",
              "  Name: count, dtype: int64},\n",
              " '600009': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  1    17\n",
              "  2    14\n",
              "  Name: count, dtype: int64},\n",
              " '600010': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  0    24\n",
              "  2    18\n",
              "  1     7\n",
              "  Name: count, dtype: int64},\n",
              " '600011': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  2    18\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '600015': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  2    20\n",
              "  0    17\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '600016': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  2    17\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '600019': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  2    17\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '600023': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  2    15\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '600026': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    22\n",
              "  2    14\n",
              "  1    13\n",
              "  Name: count, dtype: int64},\n",
              " '600028': {'acc': 0.3469387755102041,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  2    17\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '600031': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  1    19\n",
              "  2    11\n",
              "  Name: count, dtype: int64},\n",
              " '600036': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  2    16\n",
              "  1    15\n",
              "  Name: count, dtype: int64},\n",
              " '600039': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  2    19\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '600050': {'acc': 0.3469387755102041,\n",
              "  'v_counts': Label\n",
              "  2    19\n",
              "  0    15\n",
              "  1    15\n",
              "  Name: count, dtype: int64},\n",
              " '600061': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  2    21\n",
              "  0    17\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '600085': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  2    17\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '600089': {'acc': 0.5306122448979592,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    17\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '600104': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  2    23\n",
              "  0    19\n",
              "  1     7\n",
              "  Name: count, dtype: int64},\n",
              " '600115': {'acc': 0.32653061224489793,\n",
              "  'v_counts': Label\n",
              "  1    18\n",
              "  2    16\n",
              "  0    15\n",
              "  Name: count, dtype: int64},\n",
              " '600150': {'acc': 0.425531914893617,\n",
              "  'v_counts': Label\n",
              "  1    21\n",
              "  0    15\n",
              "  2    11\n",
              "  Name: count, dtype: int64},\n",
              " '600160': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  2    20\n",
              "  1    16\n",
              "  0    13\n",
              "  Name: count, dtype: int64},\n",
              " '600161': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  1    17\n",
              "  2    14\n",
              "  Name: count, dtype: int64},\n",
              " '600176': {'acc': 0.4897959183673469,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    16\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '600183': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  2    20\n",
              "  0    17\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '600188': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  2    16\n",
              "  1    15\n",
              "  Name: count, dtype: int64},\n",
              " '600196': {'acc': 0.3469387755102041,\n",
              "  'v_counts': Label\n",
              "  1    21\n",
              "  2    14\n",
              "  0    14\n",
              "  Name: count, dtype: int64},\n",
              " '600219': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    16\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '600233': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  1    15\n",
              "  2    11\n",
              "  Name: count, dtype: int64},\n",
              " '600276': {'acc': 0.4897959183673469,\n",
              "  'v_counts': Label\n",
              "  0    22\n",
              "  1    14\n",
              "  2    13\n",
              "  Name: count, dtype: int64},\n",
              " '600332': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  1    17\n",
              "  0    16\n",
              "  2    16\n",
              "  Name: count, dtype: int64},\n",
              " '600346': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  1    15\n",
              "  2    15\n",
              "  Name: count, dtype: int64},\n",
              " '600362': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  1    17\n",
              "  2    14\n",
              "  Name: count, dtype: int64},\n",
              " '600372': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  2    19\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '600377': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  2    23\n",
              "  0    16\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '600415': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  2    17\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '600436': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  1    16\n",
              "  2    13\n",
              "  Name: count, dtype: int64},\n",
              " '600438': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  2    20\n",
              "  0    20\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '600460': {'acc': 0.5918367346938775,\n",
              "  'v_counts': Label\n",
              "  2    21\n",
              "  0    20\n",
              "  1     8\n",
              "  Name: count, dtype: int64},\n",
              " '600482': {'acc': 0.425531914893617,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  2    18\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '600489': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  1    17\n",
              "  2    12\n",
              "  Name: count, dtype: int64},\n",
              " '600515': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  2    17\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '600519': {'acc': 0.32653061224489793,\n",
              "  'v_counts': Label\n",
              "  1    21\n",
              "  0    15\n",
              "  2    13\n",
              "  Name: count, dtype: int64},\n",
              " '600570': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  2    18\n",
              "  1    13\n",
              "  Name: count, dtype: int64},\n",
              " '600584': {'acc': 0.5,\n",
              "  'v_counts': Label\n",
              "  2    20\n",
              "  0    18\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '600585': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  1    18\n",
              "  2    13\n",
              "  Name: count, dtype: int64},\n",
              " '600588': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  2    20\n",
              "  0    19\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '600600': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  2    18\n",
              "  0    17\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '600660': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  2    18\n",
              "  0    16\n",
              "  1    15\n",
              "  Name: count, dtype: int64},\n",
              " '600674': {'acc': 0.32653061224489793,\n",
              "  'v_counts': Label\n",
              "  1    17\n",
              "  0    17\n",
              "  2    15\n",
              "  Name: count, dtype: int64},\n",
              " '600741': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  2    21\n",
              "  0    15\n",
              "  1    13\n",
              "  Name: count, dtype: int64},\n",
              " '600760': {'acc': 0.5918367346938775,\n",
              "  'v_counts': Label\n",
              "  0    24\n",
              "  2    21\n",
              "  1     4\n",
              "  Name: count, dtype: int64},\n",
              " '600795': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  1    15\n",
              "  2    13\n",
              "  Name: count, dtype: int64},\n",
              " '600809': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    14\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '600845': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    18\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '600887': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  2    21\n",
              "  0    17\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '600893': {'acc': 0.3469387755102041,\n",
              "  'v_counts': Label\n",
              "  2    19\n",
              "  0    18\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '600900': {'acc': 0.2653061224489796,\n",
              "  'v_counts': Label\n",
              "  2    19\n",
              "  1    18\n",
              "  0    12\n",
              "  Name: count, dtype: int64},\n",
              " '600905': {'acc': 0.32653061224489793,\n",
              "  'v_counts': Label\n",
              "  1    23\n",
              "  0    16\n",
              "  2    10\n",
              "  Name: count, dtype: int64},\n",
              " '600918': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  2    18\n",
              "  0    17\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '600919': {'acc': 0.3469387755102041,\n",
              "  'v_counts': Label\n",
              "  2    23\n",
              "  0    13\n",
              "  1    13\n",
              "  Name: count, dtype: int64},\n",
              " '600938': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  2    17\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '600941': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  1    20\n",
              "  2    17\n",
              "  0    12\n",
              "  Name: count, dtype: int64},\n",
              " '600958': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  2    19\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '600999': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  1    15\n",
              "  2    14\n",
              "  Name: count, dtype: int64},\n",
              " '601006': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  1    24\n",
              "  2    14\n",
              "  0    11\n",
              "  Name: count, dtype: int64},\n",
              " '601009': {'acc': 0.2857142857142857,\n",
              "  'v_counts': Label\n",
              "  1    22\n",
              "  2    15\n",
              "  0    12\n",
              "  Name: count, dtype: int64},\n",
              " '601012': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  1    19\n",
              "  0    18\n",
              "  2    12\n",
              "  Name: count, dtype: int64},\n",
              " '601021': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  2    16\n",
              "  1    15\n",
              "  Name: count, dtype: int64},\n",
              " '601058': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  2    18\n",
              "  0    17\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '601059': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  2    20\n",
              "  0    17\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '601066': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  2    16\n",
              "  1    13\n",
              "  Name: count, dtype: int64},\n",
              " '601077': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  2    17\n",
              "  1    16\n",
              "  0    16\n",
              "  Name: count, dtype: int64},\n",
              " '601088': {'acc': 0.3469387755102041,\n",
              "  'v_counts': Label\n",
              "  2    20\n",
              "  0    20\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '601100': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    16\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '601111': {'acc': 0.5510204081632653,\n",
              "  'v_counts': Label\n",
              "  2    21\n",
              "  0    19\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '601117': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  2    17\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '601127': {'acc': 0.4897959183673469,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  2    20\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '601136': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  2    22\n",
              "  0    18\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '601166': {'acc': 0.32653061224489793,\n",
              "  'v_counts': Label\n",
              "  1    19\n",
              "  2    16\n",
              "  0    14\n",
              "  Name: count, dtype: int64},\n",
              " '601186': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  1    17\n",
              "  2    14\n",
              "  Name: count, dtype: int64},\n",
              " '601211': {'acc': 0.4666666666666667,\n",
              "  'v_counts': Label\n",
              "  2    16\n",
              "  0    15\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '601225': {'acc': 0.4897959183673469,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  2    17\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '601229': {'acc': 0.5306122448979592,\n",
              "  'v_counts': Label\n",
              "  2    21\n",
              "  1    15\n",
              "  0    13\n",
              "  Name: count, dtype: int64},\n",
              " '601236': {'acc': 0.32653061224489793,\n",
              "  'v_counts': Label\n",
              "  2    20\n",
              "  1    15\n",
              "  0    14\n",
              "  Name: count, dtype: int64},\n",
              " '601238': {'acc': 0.4897959183673469,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    21\n",
              "  1     5\n",
              "  Name: count, dtype: int64},\n",
              " '601288': {'acc': 0.3469387755102041,\n",
              "  'v_counts': Label\n",
              "  1    18\n",
              "  2    17\n",
              "  0    14\n",
              "  Name: count, dtype: int64},\n",
              " '601318': {'acc': 0.2653061224489796,\n",
              "  'v_counts': Label\n",
              "  1    18\n",
              "  0    18\n",
              "  2    13\n",
              "  Name: count, dtype: int64},\n",
              " '601319': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  0    17\n",
              "  2    17\n",
              "  1    15\n",
              "  Name: count, dtype: int64},\n",
              " '601360': {'acc': 0.4897959183673469,\n",
              "  'v_counts': Label\n",
              "  0    22\n",
              "  2    21\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '601390': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  1    16\n",
              "  2    15\n",
              "  Name: count, dtype: int64},\n",
              " '601398': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  1    20\n",
              "  2    17\n",
              "  0    12\n",
              "  Name: count, dtype: int64},\n",
              " '601600': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  2    15\n",
              "  1    15\n",
              "  Name: count, dtype: int64},\n",
              " '601601': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  0    24\n",
              "  2    14\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '601607': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  2    17\n",
              "  1    13\n",
              "  Name: count, dtype: int64},\n",
              " '601618': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  2    20\n",
              "  0    17\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '601628': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    16\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '601658': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  2    19\n",
              "  0    17\n",
              "  1    13\n",
              "  Name: count, dtype: int64},\n",
              " '601668': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  1    15\n",
              "  2    15\n",
              "  Name: count, dtype: int64},\n",
              " '601669': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  2    17\n",
              "  1    13\n",
              "  Name: count, dtype: int64},\n",
              " '601698': {'acc': 0.5714285714285714,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    20\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '601728': {'acc': 0.32653061224489793,\n",
              "  'v_counts': Label\n",
              "  1    17\n",
              "  2    17\n",
              "  0    15\n",
              "  Name: count, dtype: int64},\n",
              " '601766': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  2    19\n",
              "  0    16\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '601788': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  2    19\n",
              "  0    17\n",
              "  1    13\n",
              "  Name: count, dtype: int64},\n",
              " '601799': {'acc': 0.4897959183673469,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    18\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '601800': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  0    20\n",
              "  2    16\n",
              "  1    13\n",
              "  Name: count, dtype: int64},\n",
              " '601808': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  2    19\n",
              "  0    16\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '601816': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  2    22\n",
              "  0    20\n",
              "  1     7\n",
              "  Name: count, dtype: int64},\n",
              " '601825': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  2    21\n",
              "  0    14\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '601838': {'acc': 0.3469387755102041,\n",
              "  'v_counts': Label\n",
              "  0    17\n",
              "  2    17\n",
              "  1    15\n",
              "  Name: count, dtype: int64},\n",
              " '601857': {'acc': 0.30612244897959184,\n",
              "  'v_counts': Label\n",
              "  1    19\n",
              "  0    16\n",
              "  2    14\n",
              "  Name: count, dtype: int64},\n",
              " '601865': {'acc': 0.5510204081632653,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    18\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '601868': {'acc': 0.32653061224489793,\n",
              "  'v_counts': Label\n",
              "  1    23\n",
              "  0    15\n",
              "  2    11\n",
              "  Name: count, dtype: int64},\n",
              " '601872': {'acc': 0.30612244897959184,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    17\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '601881': {'acc': 0.2857142857142857,\n",
              "  'v_counts': Label\n",
              "  2    19\n",
              "  0    16\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '601888': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    17\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '601898': {'acc': 0.3469387755102041,\n",
              "  'v_counts': Label\n",
              "  0    18\n",
              "  2    17\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '601901': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  0    17\n",
              "  1    16\n",
              "  2    16\n",
              "  Name: count, dtype: int64},\n",
              " '601916': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  2    18\n",
              "  0    16\n",
              "  1    15\n",
              "  Name: count, dtype: int64},\n",
              " '601919': {'acc': 0.3877551020408163,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  2    18\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '601939': {'acc': 0.40816326530612246,\n",
              "  'v_counts': Label\n",
              "  1    19\n",
              "  2    16\n",
              "  0    14\n",
              "  Name: count, dtype: int64},\n",
              " '601985': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    18\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '601988': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  1    24\n",
              "  2    16\n",
              "  0     9\n",
              "  Name: count, dtype: int64},\n",
              " '601995': {'acc': 0.30612244897959184,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  2    16\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '603019': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  2    21\n",
              "  0    20\n",
              "  1     8\n",
              "  Name: count, dtype: int64},\n",
              " '603195': {'acc': 0.32653061224489793,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  1    16\n",
              "  2    14\n",
              "  Name: count, dtype: int64},\n",
              " '603259': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    19\n",
              "  1     9\n",
              "  Name: count, dtype: int64},\n",
              " '603260': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  2    16\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '603296': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  2    22\n",
              "  0    20\n",
              "  1     7\n",
              "  Name: count, dtype: int64},\n",
              " '603392': {'acc': 0.5306122448979592,\n",
              "  'v_counts': Label\n",
              "  0    26\n",
              "  2    17\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '603799': {'acc': 0.3469387755102041,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    15\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '603806': {'acc': 0.5510204081632653,\n",
              "  'v_counts': Label\n",
              "  0    26\n",
              "  2    13\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '603833': {'acc': 0.30612244897959184,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  2    16\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '603986': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  2    23\n",
              "  0    20\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '603993': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  1    15\n",
              "  2    13\n",
              "  Name: count, dtype: int64},\n",
              " '605117': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  0    24\n",
              "  2    18\n",
              "  1     7\n",
              "  Name: count, dtype: int64},\n",
              " '605499': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  2    21\n",
              "  0    15\n",
              "  1    13\n",
              "  Name: count, dtype: int64},\n",
              " '688008': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    22\n",
              "  2    21\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '688036': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  0    26\n",
              "  2    20\n",
              "  1     3\n",
              "  Name: count, dtype: int64},\n",
              " '688041': {'acc': 0.5102040816326531,\n",
              "  'v_counts': Label\n",
              "  2    23\n",
              "  0    20\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '688082': {'acc': 0.5510204081632653,\n",
              "  'v_counts': Label\n",
              "  0    23\n",
              "  2    18\n",
              "  1     8\n",
              "  Name: count, dtype: int64},\n",
              " '688111': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    24\n",
              "  2    18\n",
              "  1     7\n",
              "  Name: count, dtype: int64},\n",
              " '688126': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  0    24\n",
              "  2    13\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '688169': {'acc': 0.42857142857142855,\n",
              "  'v_counts': Label\n",
              "  0    24\n",
              "  2    13\n",
              "  1    12\n",
              "  Name: count, dtype: int64},\n",
              " '688187': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    19\n",
              "  2    16\n",
              "  1    14\n",
              "  Name: count, dtype: int64},\n",
              " '688223': {'acc': 0.5306122448979592,\n",
              "  'v_counts': Label\n",
              "  0    26\n",
              "  2    13\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '688256': {'acc': 0.46938775510204084,\n",
              "  'v_counts': Label\n",
              "  2    24\n",
              "  0    18\n",
              "  1     7\n",
              "  Name: count, dtype: int64},\n",
              " '688271': {'acc': 0.3673469387755102,\n",
              "  'v_counts': Label\n",
              "  2    21\n",
              "  0    20\n",
              "  1     8\n",
              "  Name: count, dtype: int64},\n",
              " '688303': {'acc': 0.4897959183673469,\n",
              "  'v_counts': Label\n",
              "  0    21\n",
              "  2    17\n",
              "  1    11\n",
              "  Name: count, dtype: int64},\n",
              " '688396': {'acc': 0.4897959183673469,\n",
              "  'v_counts': Label\n",
              "  0    26\n",
              "  2    13\n",
              "  1    10\n",
              "  Name: count, dtype: int64},\n",
              " '688472': {'acc': 0.5510204081632653,\n",
              "  'v_counts': Label\n",
              "  0    25\n",
              "  2    18\n",
              "  1     6\n",
              "  Name: count, dtype: int64},\n",
              " '688506': {'acc': 0.5306122448979592,\n",
              "  'v_counts': Label\n",
              "  0    26\n",
              "  2    18\n",
              "  1     5\n",
              "  Name: count, dtype: int64},\n",
              " '688981': {'acc': 0.4489795918367347,\n",
              "  'v_counts': Label\n",
              "  0    25\n",
              "  2    20\n",
              "  1     4\n",
              "  Name: count, dtype: int64}}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stock_accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20374fe9",
      "metadata": {
        "id": "20374fe9",
        "outputId": "7841f917-a543-444e-99a7-f59d37db1b69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "002920: 0.6326530612244898 0.5306122448979592\n",
            "300782: 0.5918367346938775 0.5510204081632653\n",
            "600460: 0.5918367346938775 0.42857142857142855\n",
            "600760: 0.5918367346938775 0.4897959183673469\n",
            "300015: 0.5714285714285714 0.5102040816326531\n",
            "601698: 0.5714285714285714 0.46938775510204084\n",
            "002230: 0.5510204081632653 0.42857142857142855\n",
            "300347: 0.5510204081632653 0.5510204081632653\n",
            "300394: 0.5510204081632653 0.4897959183673469\n",
            "300502: 0.5510204081632653 0.4489795918367347\n",
            "601111: 0.5510204081632653 0.42857142857142855\n",
            "601865: 0.5510204081632653 0.42857142857142855\n",
            "603806: 0.5510204081632653 0.5306122448979592\n",
            "688082: 0.5510204081632653 0.46938775510204084\n",
            "688472: 0.5510204081632653 0.5102040816326531\n",
            "000977: 0.5306122448979592 0.4489795918367347\n",
            "002027: 0.5306122448979592 0.4489795918367347\n",
            "002142: 0.5306122448979592 0.42857142857142855\n",
            "002180: 0.5306122448979592 0.4489795918367347\n",
            "002236: 0.5306122448979592 0.46938775510204084\n",
            "002371: 0.5306122448979592 0.4489795918367347\n",
            "300628: 0.5306122448979592 0.4489795918367347\n",
            "300661: 0.5306122448979592 0.42857142857142855\n",
            "300832: 0.5306122448979592 0.42857142857142855\n",
            "600089: 0.5306122448979592 0.42857142857142855\n",
            "601229: 0.5306122448979592 0.42857142857142855\n",
            "603392: 0.5306122448979592 0.5306122448979592\n",
            "688223: 0.5306122448979592 0.5306122448979592\n",
            "688506: 0.5306122448979592 0.5306122448979592\n",
            "000002: 0.5102040816326531 0.46938775510204084\n",
            "000596: 0.5102040816326531 0.46938775510204084\n",
            "000768: 0.5102040816326531 0.46938775510204084\n",
            "002493: 0.5102040816326531 0.40816326530612246\n",
            "002648: 0.5102040816326531 0.40816326530612246\n",
            "002916: 0.5102040816326531 0.42857142857142855\n",
            "300014: 0.5102040816326531 0.46938775510204084\n",
            "300408: 0.5102040816326531 0.4489795918367347\n",
            "300999: 0.5102040816326531 0.4897959183673469\n",
            "301269: 0.5102040816326531 0.4489795918367347\n",
            "600010: 0.5102040816326531 0.4897959183673469\n",
            "600438: 0.5102040816326531 0.40816326530612246\n",
            "600809: 0.5102040816326531 0.46938775510204084\n",
            "600938: 0.5102040816326531 0.40816326530612246\n",
            "601059: 0.5102040816326531 0.40816326530612246\n",
            "601601: 0.5102040816326531 0.4897959183673469\n",
            "603019: 0.5102040816326531 0.42857142857142855\n",
            "688041: 0.5102040816326531 0.46938775510204084\n",
            "600584: 0.5 0.4166666666666667\n",
            "000999: 0.4897959183673469 0.4489795918367347\n",
            "002938: 0.4897959183673469 0.4489795918367347\n",
            "300033: 0.4897959183673469 0.42857142857142855\n",
            "300274: 0.4897959183673469 0.5306122448979592\n",
            "300418: 0.4897959183673469 0.46938775510204084\n",
            "600176: 0.4897959183673469 0.42857142857142855\n",
            "600276: 0.4897959183673469 0.4489795918367347\n",
            "601127: 0.4897959183673469 0.40816326530612246\n",
            "601225: 0.4897959183673469 0.40816326530612246\n",
            "601238: 0.4897959183673469 0.46938775510204084\n",
            "601360: 0.4897959183673469 0.4489795918367347\n",
            "601799: 0.4897959183673469 0.42857142857142855\n",
            "688303: 0.4897959183673469 0.42857142857142855\n",
            "688396: 0.4897959183673469 0.5306122448979592\n",
            "000568: 0.46938775510204084 0.46938775510204084\n",
            "000708: 0.46938775510204084 0.3877551020408163\n",
            "000876: 0.46938775510204084 0.4897959183673469\n",
            "002600: 0.46938775510204084 0.46938775510204084\n",
            "002714: 0.46938775510204084 0.40816326530612246\n",
            "300308: 0.46938775510204084 0.4897959183673469\n",
            "300760: 0.46938775510204084 0.40816326530612246\n",
            "600015: 0.46938775510204084 0.40816326530612246\n",
            "600023: 0.46938775510204084 0.40816326530612246\n",
            "600183: 0.46938775510204084 0.40816326530612246\n",
            "600436: 0.46938775510204084 0.40816326530612246\n",
            "600588: 0.46938775510204084 0.40816326530612246\n",
            "600741: 0.46938775510204084 0.42857142857142855\n",
            "601077: 0.46938775510204084 0.3469387755102041\n",
            "601136: 0.46938775510204084 0.4489795918367347\n",
            "601816: 0.46938775510204084 0.4489795918367347\n",
            "601988: 0.46938775510204084 0.4897959183673469\n",
            "603296: 0.46938775510204084 0.4489795918367347\n",
            "605117: 0.46938775510204084 0.4897959183673469\n",
            "605499: 0.46938775510204084 0.42857142857142855\n",
            "688036: 0.46938775510204084 0.5306122448979592\n",
            "688126: 0.46938775510204084 0.4897959183673469\n",
            "688256: 0.46938775510204084 0.4897959183673469\n",
            "601211: 0.4666666666666667 0.35555555555555557\n",
            "000063: 0.4489795918367347 0.42857142857142855\n",
            "000661: 0.4489795918367347 0.4897959183673469\n",
            "000786: 0.4489795918367347 0.46938775510204084\n",
            "001289: 0.4489795918367347 0.5102040816326531\n",
            "001979: 0.4489795918367347 0.42857142857142855\n",
            "002049: 0.4489795918367347 0.40816326530612246\n",
            "002050: 0.4489795918367347 0.4897959183673469\n",
            "002179: 0.4489795918367347 0.4489795918367347\n",
            "002241: 0.4489795918367347 0.46938775510204084\n",
            "002352: 0.4489795918367347 0.40816326530612246\n",
            "002601: 0.4489795918367347 0.42857142857142855\n",
            "300059: 0.4489795918367347 0.46938775510204084\n",
            "300316: 0.4489795918367347 0.42857142857142855\n",
            "300433: 0.4489795918367347 0.4897959183673469\n",
            "300759: 0.4489795918367347 0.5102040816326531\n",
            "600104: 0.4489795918367347 0.46938775510204084\n",
            "600219: 0.4489795918367347 0.42857142857142855\n",
            "600233: 0.4489795918367347 0.46938775510204084\n",
            "600377: 0.4489795918367347 0.46938775510204084\n",
            "600415: 0.4489795918367347 0.3673469387755102\n",
            "601100: 0.4489795918367347 0.46938775510204084\n",
            "601607: 0.4489795918367347 0.3877551020408163\n",
            "601825: 0.4489795918367347 0.42857142857142855\n",
            "603259: 0.4489795918367347 0.42857142857142855\n",
            "603986: 0.4489795918367347 0.46938775510204084\n",
            "603993: 0.4489795918367347 0.42857142857142855\n",
            "688008: 0.4489795918367347 0.4489795918367347\n",
            "688111: 0.4489795918367347 0.4897959183673469\n",
            "688187: 0.4489795918367347 0.3877551020408163\n",
            "688981: 0.4489795918367347 0.5102040816326531\n",
            "000807: 0.42857142857142855 0.4489795918367347\n",
            "002311: 0.42857142857142855 0.3877551020408163\n",
            "002422: 0.42857142857142855 0.42857142857142855\n",
            "002460: 0.42857142857142855 0.4489795918367347\n",
            "002475: 0.42857142857142855 0.5102040816326531\n",
            "600009: 0.42857142857142855 0.3673469387755102\n",
            "600016: 0.42857142857142855 0.3673469387755102\n",
            "600026: 0.42857142857142855 0.4489795918367347\n",
            "600161: 0.42857142857142855 0.3673469387755102\n",
            "600188: 0.42857142857142855 0.3673469387755102\n",
            "600346: 0.42857142857142855 0.3877551020408163\n",
            "600918: 0.42857142857142855 0.3673469387755102\n",
            "600958: 0.42857142857142855 0.3877551020408163\n",
            "601066: 0.42857142857142855 0.40816326530612246\n",
            "601600: 0.42857142857142855 0.3877551020408163\n",
            "601618: 0.42857142857142855 0.40816326530612246\n",
            "601628: 0.42857142857142855 0.42857142857142855\n",
            "601668: 0.42857142857142855 0.3877551020408163\n",
            "601669: 0.42857142857142855 0.3877551020408163\n",
            "601766: 0.42857142857142855 0.3877551020408163\n",
            "601788: 0.42857142857142855 0.3877551020408163\n",
            "601888: 0.42857142857142855 0.42857142857142855\n",
            "688169: 0.42857142857142855 0.4897959183673469\n",
            "600150: 0.425531914893617 0.44680851063829785\n",
            "600482: 0.425531914893617 0.40425531914893614\n",
            "000301: 0.40816326530612246 0.4489795918367347\n",
            "000630: 0.40816326530612246 0.3877551020408163\n",
            "000792: 0.40816326530612246 0.42857142857142855\n",
            "000938: 0.40816326530612246 0.46938775510204084\n",
            "002304: 0.40816326530612246 0.3469387755102041\n",
            "301236: 0.40816326530612246 0.42857142857142855\n",
            "600011: 0.40816326530612246 0.40816326530612246\n",
            "600362: 0.40816326530612246 0.3673469387755102\n",
            "600489: 0.40816326530612246 0.40816326530612246\n",
            "600585: 0.40816326530612246 0.3673469387755102\n",
            "600600: 0.40816326530612246 0.3673469387755102\n",
            "600845: 0.40816326530612246 0.42857142857142855\n",
            "601006: 0.40816326530612246 0.4897959183673469\n",
            "601012: 0.40816326530612246 0.3877551020408163\n",
            "601058: 0.40816326530612246 0.3673469387755102\n",
            "601186: 0.40816326530612246 0.3673469387755102\n",
            "601319: 0.40816326530612246 0.3469387755102041\n",
            "601398: 0.40816326530612246 0.40816326530612246\n",
            "601901: 0.40816326530612246 0.3469387755102041\n",
            "601916: 0.40816326530612246 0.3673469387755102\n",
            "601939: 0.40816326530612246 0.3877551020408163\n",
            "000895: 0.3877551020408163 0.3469387755102041\n",
            "002466: 0.3877551020408163 0.3877551020408163\n",
            "600061: 0.3877551020408163 0.42857142857142855\n",
            "600085: 0.3877551020408163 0.3673469387755102\n",
            "600160: 0.3877551020408163 0.40816326530612246\n",
            "600332: 0.3877551020408163 0.3469387755102041\n",
            "600372: 0.3877551020408163 0.40816326530612246\n",
            "600515: 0.3877551020408163 0.40816326530612246\n",
            "600570: 0.3877551020408163 0.3673469387755102\n",
            "600660: 0.3877551020408163 0.3673469387755102\n",
            "600795: 0.3877551020408163 0.42857142857142855\n",
            "600887: 0.3877551020408163 0.42857142857142855\n",
            "600941: 0.3877551020408163 0.40816326530612246\n",
            "600999: 0.3877551020408163 0.40816326530612246\n",
            "601658: 0.3877551020408163 0.3877551020408163\n",
            "601800: 0.3877551020408163 0.40816326530612246\n",
            "601919: 0.3877551020408163 0.3877551020408163\n",
            "002252: 0.3829787234042553 0.3829787234042553\n",
            "000001: 0.3673469387755102 0.3469387755102041\n",
            "000333: 0.3673469387755102 0.3469387755102041\n",
            "000651: 0.3673469387755102 0.3877551020408163\n",
            "000800: 0.3673469387755102 0.3877551020408163\n",
            "002001: 0.3673469387755102 0.3877551020408163\n",
            "300750: 0.3673469387755102 0.42857142857142855\n",
            "600019: 0.3673469387755102 0.3673469387755102\n",
            "600031: 0.3673469387755102 0.3877551020408163\n",
            "600036: 0.3673469387755102 0.3673469387755102\n",
            "600039: 0.3673469387755102 0.3877551020408163\n",
            "601021: 0.3673469387755102 0.3673469387755102\n",
            "601117: 0.3673469387755102 0.3673469387755102\n",
            "601390: 0.3673469387755102 0.3673469387755102\n",
            "601808: 0.3673469387755102 0.3877551020408163\n",
            "601985: 0.3673469387755102 0.42857142857142855\n",
            "603260: 0.3673469387755102 0.3877551020408163\n",
            "688271: 0.3673469387755102 0.42857142857142855\n",
            "000983: 0.3469387755102041 0.46938775510204084\n",
            "600028: 0.3469387755102041 0.3673469387755102\n",
            "600050: 0.3469387755102041 0.3877551020408163\n",
            "600196: 0.3469387755102041 0.42857142857142855\n",
            "600893: 0.3469387755102041 0.3877551020408163\n",
            "600919: 0.3469387755102041 0.46938775510204084\n",
            "601088: 0.3469387755102041 0.40816326530612246\n",
            "601288: 0.3469387755102041 0.3673469387755102\n",
            "601838: 0.3469387755102041 0.3469387755102041\n",
            "601898: 0.3469387755102041 0.3673469387755102\n",
            "603799: 0.3469387755102041 0.46938775510204084\n",
            "000338: 0.32653061224489793 0.3673469387755102\n",
            "600000: 0.32653061224489793 0.3673469387755102\n",
            "600115: 0.32653061224489793 0.3673469387755102\n",
            "600519: 0.32653061224489793 0.42857142857142855\n",
            "600674: 0.32653061224489793 0.3469387755102041\n",
            "600905: 0.32653061224489793 0.46938775510204084\n",
            "601166: 0.32653061224489793 0.3877551020408163\n",
            "601236: 0.32653061224489793 0.40816326530612246\n",
            "601728: 0.32653061224489793 0.3469387755102041\n",
            "601868: 0.32653061224489793 0.46938775510204084\n",
            "603195: 0.32653061224489793 0.3877551020408163\n",
            "003816: 0.30612244897959184 0.3469387755102041\n",
            "601857: 0.30612244897959184 0.3877551020408163\n",
            "601872: 0.30612244897959184 0.42857142857142855\n",
            "601995: 0.30612244897959184 0.3877551020408163\n",
            "603833: 0.30612244897959184 0.3877551020408163\n",
            "000776: 0.2857142857142857 0.3877551020408163\n",
            "601009: 0.2857142857142857 0.4489795918367347\n",
            "601881: 0.2857142857142857 0.3877551020408163\n",
            "000963: 0.2653061224489796 0.3469387755102041\n",
            "600900: 0.2653061224489796 0.3877551020408163\n",
            "601318: 0.2653061224489796 0.3673469387755102\n"
          ]
        }
      ],
      "source": [
        "for code, v in sorted(stock_accs.items(), key=lambda item: item[1]['acc'], reverse=True):\n",
        "    print(f\"{code}: {v['acc']} {max(v['v_counts']) / sum(v['v_counts'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "095fd855",
      "metadata": {
        "id": "095fd855",
        "outputId": "d2d3bb55-7d23-448b-ca90-afe3b838cade"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "99"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(all_in_one_file[all_in_one_file['StockCode'] == '600089']['Label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PatchTST"
      ],
      "metadata": {
        "id": "ceFHWiRwz35A"
      },
      "id": "ceFHWiRwz35A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1500b198",
      "metadata": {
        "id": "1500b198"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ğŸš€ Loop fine-tuning across all StockCodes & rank top-10 by metrics\n",
        "#   NOTE* - RUN ON GOOGLE COLAB\n",
        "# ============================================\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install \"transformers>=4.46.0\" \"accelerate>=0.34.0\" pyarrow tqdm scikit-learn pandas numpy\n",
        "\n",
        "import re, os, math, pandas as pd, numpy as np, torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from transformers import PatchTSTModel\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# ============================================\n",
        "# ğŸ“‚ Paths / globals\n",
        "# ============================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PATH_PARQUET   = \"/content/drive/MyDrive/merged.parquet\"\n",
        "BASE_CKPT      = \"/content/drive/MyDrive/best_model.pt\"        # previously trained general ckpt\n",
        "OUT_DIR        = \"/content/drive/MyDrive/ft_per_code\"          # where per-code ckpts & CSV go\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ============================================\n",
        "# âš™ï¸ Common hyperparams for per-code FT\n",
        "# ============================================\n",
        "window_size     = 90         # try 60/30/90 if you'd like to sweep\n",
        "train_ratio     = 0.85\n",
        "batch_size      = 64\n",
        "num_classes     = 3\n",
        "num_epochs      = 15         # keep modest for 230 codes; early stopping included\n",
        "patience        = 3          # early stop on val loss\n",
        "warmup_epochs   = 2\n",
        "\n",
        "# Optim (head-only to start; set UNFREEZE_EPOCH to lightly unfreeze later)\n",
        "lr_head         = 5e-4\n",
        "lr_backbone     = 1e-6\n",
        "weight_decay    = 5e-4\n",
        "grad_clip       = 1.0\n",
        "UNFREEZE_EPOCH  = None       # e.g., 6 to unfreeze last block lightly\n",
        "\n",
        "label_smoothing = 0.10\n",
        "USE_CLASS_WEIGHTS = True\n",
        "\n",
        "# Granite PatchTST dims\n",
        "BACKBONE_ID       = \"ibm-granite/granite-timeseries-patchtst\"\n",
        "proj_dim          = 128\n",
        "hidden_fused      = 512\n",
        "granite_channels  = 7\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# ============================================\n",
        "# ğŸ§© Columns (emotion + stock)\n",
        "# ============================================\n",
        "emotion_cols = [\n",
        "    'Emotion_Index_closing',\n",
        "    'Emotion_Index_trading',\n",
        "    'Emotion_Momentum_3d_closing',\n",
        "    'Emotion_Momentum_3d_trading'\n",
        "]\n",
        "stock_cols = ['log_return', 'amplitude']\n",
        "feature_cols = emotion_cols + stock_cols\n",
        "LABEL_COL = \"Label\"\n",
        "CODE_COL  = \"StockCode\"\n",
        "DATE_COL  = \"Date\"\n",
        "\n",
        "# ============================================\n",
        "# ğŸ§  Model & helpers (same architecture as before)\n",
        "# ============================================\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, hidden_size, bottleneck=64, dropout=0.05, scale=1.0):\n",
        "        super().__init__()\n",
        "        self.ln = nn.LayerNorm(hidden_size)\n",
        "        self.down = nn.Linear(hidden_size, bottleneck)\n",
        "        self.up   = nn.Linear(bottleneck, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = scale\n",
        "        nn.init.kaiming_uniform_(self.down.weight, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.down.bias); nn.init.zeros_(self.up.weight); nn.init.zeros_(self.up.bias)\n",
        "    def forward(self, x):\n",
        "        z = self.ln(x); z = torch.relu(self.down(z)); z = self.dropout(z); z = self.up(z)\n",
        "        return x + self.scale * z\n",
        "\n",
        "def right_pad_to_context(x, context_len, pad_value=0.0):\n",
        "    B,T,C = x.shape\n",
        "    if T > context_len:\n",
        "        x = x[:, -context_len:, :]; T = context_len\n",
        "    pad_len = context_len - T\n",
        "    if pad_len > 0:\n",
        "        pad = x.new_full((B,pad_len,C), pad_value)\n",
        "        x = torch.cat([pad, x], dim=1)\n",
        "        mask = torch.cat([torch.zeros(B,pad_len,C, device=x.device),\n",
        "                          torch.ones(B,T,C, device=x.device)], dim=1)\n",
        "    else:\n",
        "        mask = torch.ones(B,T,C, device=x.device)\n",
        "    return x, mask\n",
        "\n",
        "def sanitize_logits(logits, num_classes):\n",
        "    if logits.dim()==3:\n",
        "        if logits.size(-1)==num_classes: logits = logits.mean(dim=1)\n",
        "        elif logits.size(1)==num_classes: logits = logits.mean(dim=2)\n",
        "        else: logits = logits.mean(dim=1)\n",
        "    return logits\n",
        "\n",
        "class EmoStockToGranite(nn.Module):\n",
        "    def __init__(self, stock_dim=2, emo_dim=4,\n",
        "                 proj_dim=128, fused_dim=512, granite_channels=7,\n",
        "                 num_classes=3, backbone_id=BACKBONE_ID):\n",
        "        super().__init__()\n",
        "        self.stock_proj = nn.Sequential(nn.Linear(stock_dim, proj_dim), nn.ReLU(), nn.LayerNorm(proj_dim))\n",
        "        self.emo_proj   = nn.Sequential(nn.LayerNorm(emo_dim), nn.Linear(emo_dim, proj_dim), nn.ReLU(), nn.LayerNorm(proj_dim))\n",
        "        self.fuse       = nn.Sequential(nn.Linear(proj_dim*2, fused_dim), nn.ReLU(), nn.LayerNorm(fused_dim))\n",
        "        self.channel_aligner = nn.Linear(fused_dim, granite_channels)\n",
        "\n",
        "        self.backbone = PatchTSTModel.from_pretrained(backbone_id)\n",
        "        self.context_len = int(self.backbone.config.context_length)\n",
        "        d_model = int(self.backbone.config.d_model)\n",
        "\n",
        "        for p in self.backbone.parameters():\n",
        "            p.requires_grad = False  # head warmup by default\n",
        "\n",
        "        self.adapter  = Adapter(d_model, bottleneck=64, dropout=0.05)\n",
        "        self.cls_head = nn.Sequential(nn.Linear(d_model, d_model//2), nn.ReLU(), nn.Dropout(0.1), nn.Linear(d_model//2, num_classes))\n",
        "\n",
        "    def forward(self, stock_seq, emo_seq):\n",
        "        s = self.stock_proj(stock_seq)                      # [B,T,P]\n",
        "        e = self.emo_proj(emo_seq)                          # [B,T,P]\n",
        "        fused = self.fuse(torch.cat([s,e], dim=-1))         # [B,T,F]\n",
        "        pv = self.channel_aligner(fused)                    # [B,T,C]\n",
        "        pv, obs_mask = right_pad_to_context(pv, self.context_len)\n",
        "        out = self.backbone(past_values=pv, past_observed_mask=obs_mask.to(dtype=torch.bool, device=pv.device), output_hidden_states=True)\n",
        "        hs = self.adapter(out.last_hidden_state)            # [B,context_len,d_model]\n",
        "        pooled = hs.mean(dim=1)                             # mean pooling\n",
        "        return self.cls_head(pooled)                        # [B,num_classes]\n",
        "\n",
        "def unfreeze_top_blocks(mdl, n_blocks=1):\n",
        "    names = [n for n,_ in mdl.backbone.named_parameters()]\n",
        "    layer_idx = []\n",
        "    for n in names:\n",
        "        m = re.search(r\"layers\\.(\\d+)\", n)\n",
        "        if m: layer_idx.append(int(m.group(1)))\n",
        "    changed = 0\n",
        "    if layer_idx:\n",
        "        max_idx = max(layer_idx)\n",
        "        target = set(range(max_idx - (n_blocks-1), max_idx + 1))\n",
        "        for n,p in mdl.backbone.named_parameters():\n",
        "            m = re.search(r\"layers\\.(\\d+)\", n)\n",
        "            if m and int(m.group(1)) in target:\n",
        "                p.requires_grad = True; changed += 1\n",
        "    else:\n",
        "        for p in mdl.backbone.parameters(): p.requires_grad = True\n",
        "        changed = -1\n",
        "    return changed\n",
        "\n",
        "def decay_filter(n, p):\n",
        "    return p.requires_grad and (p.dim() > 1) and (\"bias\" not in n) and (\"LayerNorm\" not in n) and (\"layer_norm\" not in n)\n",
        "\n",
        "def build_param_groups(model, lr_head, lr_backbone, wd):\n",
        "    backbone_decay, backbone_nodecay, head_decay, head_nodecay = [], [], [], []\n",
        "    for n,p in model.named_parameters():\n",
        "        if not p.requires_grad: continue\n",
        "        is_backbone = n.startswith(\"backbone.\")\n",
        "        if decay_filter(n,p):\n",
        "            (backbone_decay if is_backbone else head_decay).append(p)\n",
        "        else:\n",
        "            (backbone_nodecay if is_backbone else head_nodecay).append(p)\n",
        "    groups = []\n",
        "    if backbone_decay:   groups.append({\"params\": backbone_decay,   \"lr\": lr_backbone, \"weight_decay\": wd})\n",
        "    if backbone_nodecay: groups.append({\"params\": backbone_nodecay, \"lr\": lr_backbone, \"weight_decay\": 0.0})\n",
        "    if head_decay:       groups.append({\"params\": head_decay,       \"lr\": lr_head,     \"weight_decay\": wd})\n",
        "    if head_nodecay:     groups.append({\"params\": head_nodecay,     \"lr\": lr_head,     \"weight_decay\": 0.0})\n",
        "    return groups\n",
        "\n",
        "def make_scheduler(optimizer, num_epochs, warmup_epochs):\n",
        "    def lr_lambda(epoch):\n",
        "        if epoch < warmup_epochs:\n",
        "            return (epoch + 1) / max(1, warmup_epochs)\n",
        "        progress = (epoch - warmup_epochs) / max(1, (num_epochs - warmup_epochs))\n",
        "        return 0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "\n",
        "# ============================================\n",
        "# ğŸ§¼ Per-code data prep\n",
        "# ============================================\n",
        "def prepare_code_windows(df_code, window_size, train_ratio):\n",
        "    # basic cleaning (single code)\n",
        "    df_code = df_code.sort_values([CODE_COL, DATE_COL]).reset_index(drop=True)\n",
        "    # replace inf, ffill/bfill\n",
        "    df_code[feature_cols] = df_code[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
        "    df_code[feature_cols] = df_code[feature_cols].ffill().bfill()\n",
        "    # fill remaining with medians\n",
        "    df_code[feature_cols] = df_code[feature_cols].fillna(df_code[feature_cols].median(numeric_only=True))\n",
        "    # robust clip & zscore\n",
        "    lo = df_code[feature_cols].quantile(0.01); hi = df_code[feature_cols].quantile(0.99)\n",
        "    df_code[feature_cols] = df_code[feature_cols].clip(lower=lo, upper=hi, axis=1)\n",
        "    mu = df_code[feature_cols].mean(); sd = df_code[feature_cols].std(ddof=0).replace(0, 1.0)\n",
        "    df_code[feature_cols] = (df_code[feature_cols] - mu) / sd\n",
        "\n",
        "    # build windows\n",
        "    samples = []\n",
        "    emo_np = df_code[emotion_cols].to_numpy(dtype=np.float32, copy=True)\n",
        "    stk_np = df_code[stock_cols].to_numpy(dtype=np.float32, copy=True)\n",
        "    emo_np[~np.isfinite(emo_np)] = np.nan\n",
        "    stk_np[~np.isfinite(stk_np)] = np.nan\n",
        "    emo = torch.from_numpy(np.nan_to_num(emo_np, nan=0.0, posinf=0.0, neginf=0.0))\n",
        "    stk = torch.from_numpy(np.nan_to_num(stk_np, nan=0.0, posinf=0.0, neginf=0.0))\n",
        "    y   = torch.tensor(df_code[LABEL_COL].values, dtype=torch.int64)\n",
        "    dt  = df_code[DATE_COL].values\n",
        "\n",
        "    for i in range(len(df_code) - window_size):\n",
        "        emos  = emo[i:i+window_size]\n",
        "        stks  = stk[i:i+window_size]\n",
        "        label = y[i + window_size]\n",
        "        if not (torch.isfinite(emos).all() and torch.isfinite(stks).all()): continue\n",
        "        samples.append({\"emotion\": emos, \"stock\": stks, \"label\": label, \"end_date\": dt[i+window_size]})\n",
        "\n",
        "    if len(samples) == 0:\n",
        "        return None, None, None, None\n",
        "\n",
        "    dates = sorted(pd.Series([s[\"end_date\"] for s in samples]).unique())\n",
        "    split_date = dates[int(len(dates) * train_ratio)]\n",
        "    train_samples = [s for s in samples if s[\"end_date\"] <= split_date]\n",
        "    val_samples   = [s for s in samples if s[\"end_date\"] >  split_date]\n",
        "\n",
        "    class WindowDataset(Dataset):\n",
        "        def __init__(self, samples): self.samples = samples\n",
        "        def __len__(self): return len(self.samples)\n",
        "        def __getitem__(self, idx): return self.samples[idx]\n",
        "    def collate_batch(batch):\n",
        "        return {\n",
        "            \"emotion\": torch.stack([b[\"emotion\"] for b in batch], dim=0),\n",
        "            \"stock\":   torch.stack([b[\"stock\"]   for b in batch], dim=0),\n",
        "            \"label\":   torch.stack([b[\"label\"]   for b in batch], dim=0),\n",
        "            \"end_date\": [pd.Timestamp(b[\"end_date\"]).to_pydatetime() for b in batch],\n",
        "        }\n",
        "\n",
        "    train_loader = DataLoader(WindowDataset(train_samples), batch_size=batch_size, shuffle=True,  collate_fn=collate_batch, pin_memory=torch.cuda.is_available())\n",
        "    val_loader   = DataLoader(WindowDataset(val_samples),   batch_size=batch_size, shuffle=False, collate_fn=collate_batch, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "    train_labels = [int(s[\"label\"]) for s in train_samples]\n",
        "    val_labels   = [int(s[\"label\"]) for s in val_samples]\n",
        "    return train_loader, val_loader, train_labels, val_labels\n",
        "\n",
        "# ============================================\n",
        "# ğŸ‹ï¸ Finetune single code & return metrics\n",
        "# ============================================\n",
        "def finetune_one_code(code, df, save_ckpt=True):\n",
        "    df_code = df[df[CODE_COL].astype(str) == code].copy()\n",
        "    if len(df_code) <= window_size + 10:\n",
        "        return {\"StockCode\": code, \"support\": 0, \"accuracy\": np.nan,\n",
        "                \"weighted_precision\": np.nan, \"weighted_recall\": np.nan, \"weighted_f1\": np.nan,\n",
        "                \"best_val_acc\": np.nan, \"best_val_loss\": np.nan, \"notes\": \"too_few_rows\"}\n",
        "\n",
        "    tl, vl, train_labels, val_labels = prepare_code_windows(df_code, window_size, train_ratio)\n",
        "    if tl is None or len(val_labels) == 0:\n",
        "        return {\"StockCode\": code, \"support\": 0, \"accuracy\": np.nan,\n",
        "                \"weighted_precision\": np.nan, \"weighted_recall\": np.nan, \"weighted_f1\": np.nan,\n",
        "                \"best_val_acc\": np.nan, \"best_val_loss\": np.nan, \"notes\": \"no_windows\"}\n",
        "\n",
        "    # Build model & load base checkpoint\n",
        "    model = EmoStockToGranite(\n",
        "        stock_dim=len(stock_cols),\n",
        "        emo_dim=len(emotion_cols),\n",
        "        proj_dim=proj_dim,\n",
        "        fused_dim=hidden_fused,\n",
        "        granite_channels=granite_channels,\n",
        "        num_classes=num_classes,\n",
        "        backbone_id=BACKBONE_ID\n",
        "    ).to(device)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(BASE_CKPT, map_location=device), strict=False)\n",
        "    except Exception as e:\n",
        "        return {\"StockCode\": code, \"support\": len(val_labels), \"accuracy\": np.nan,\n",
        "                \"weighted_precision\": np.nan, \"weighted_recall\": np.nan, \"weighted_f1\": np.nan,\n",
        "                \"best_val_acc\": np.nan, \"best_val_loss\": np.nan, \"notes\": f\"load_fail:{e}\"}\n",
        "\n",
        "    # Loss & weights per this code\n",
        "    if USE_CLASS_WEIGHTS:\n",
        "        counts = np.bincount(np.array(train_labels, dtype=int), minlength=num_classes)\n",
        "        class_weights = 1.0 / np.clip(counts, 1, None)\n",
        "        class_weights = class_weights * (num_classes / class_weights.sum())\n",
        "        class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
        "    else:\n",
        "        class_weights_tensor = None\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=label_smoothing)\n",
        "\n",
        "    # Head-only optim\n",
        "    def build_optim(m):\n",
        "        groups = build_param_groups(m, lr_head, lr_backbone, weight_decay)\n",
        "        opt = optim.AdamW(groups)\n",
        "        sch = make_scheduler(opt, num_epochs=num_epochs, warmup_epochs=warmup_epochs)\n",
        "        return opt, sch\n",
        "    optimizer, scheduler = build_optim(model)\n",
        "\n",
        "    # Optional partial unfreeze later\n",
        "    def maybe_unfreeze(epoch):\n",
        "        nonlocal optimizer, scheduler\n",
        "        if UNFREEZE_EPOCH is not None and epoch == UNFREEZE_EPOCH:\n",
        "            _ = unfreeze_top_blocks(model, n_blocks=1)\n",
        "            optimizer, scheduler = build_optim(model)\n",
        "\n",
        "    # Eval\n",
        "    def evaluate(model_for_eval, vl):\n",
        "        model_for_eval.eval()\n",
        "        val_loss = 0.0; val_correct = 0; val_total = 0\n",
        "        all_preds = []; all_true = []\n",
        "        with torch.no_grad():\n",
        "            for batch in vl:\n",
        "                emo = batch[\"emotion\"].to(device)\n",
        "                stk = batch[\"stock\"].to(device)\n",
        "                y   = batch[\"label\"].to(device)\n",
        "                logits = sanitize_logits(model_for_eval(stk, emo), num_classes)\n",
        "                loss = criterion(logits, y)\n",
        "                val_loss += loss.item() * y.size(0)\n",
        "                preds = logits.argmax(1)\n",
        "                val_correct += (preds == y).sum().item()\n",
        "                val_total += y.size(0)\n",
        "                all_preds.append(preds.detach().cpu().numpy())\n",
        "                all_true.append(y.detach().cpu().numpy())\n",
        "        acc = val_correct / max(1,val_total)\n",
        "        return (val_loss / max(1,val_total), acc, np.concatenate(all_true), np.concatenate(all_preds))\n",
        "\n",
        "    # Train with early stopping on val loss\n",
        "    best_val = float(\"inf\")\n",
        "    best_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    ckpt_path = os.path.join(OUT_DIR, f\"best_{code}.pt\")\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        maybe_unfreeze(epoch)\n",
        "        model.train()\n",
        "        train_loss = 0.0; train_correct = 0; train_total = 0\n",
        "        for batch in tl:\n",
        "            emo = batch[\"emotion\"].to(device)\n",
        "            stk = batch[\"stock\"].to(device)\n",
        "            y   = batch[\"label\"].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = sanitize_logits(model(stk, emo), num_classes)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * y.size(0)\n",
        "            train_correct += (logits.argmax(1) == y).sum().item()\n",
        "            train_total += y.size(0)\n",
        "        scheduler.step()\n",
        "\n",
        "        val_loss, val_acc, y_true, y_pred = evaluate(model, vl)\n",
        "        if val_loss < best_val - 1e-4:\n",
        "            best_val = val_loss\n",
        "            best_acc = val_acc\n",
        "            epochs_no_improve = 0\n",
        "            if save_ckpt:\n",
        "                torch.save(model.state_dict(), ckpt_path)\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                break\n",
        "\n",
        "    # Load best & compute final metrics\n",
        "    if save_ckpt and os.path.exists(ckpt_path):\n",
        "        model.load_state_dict(torch.load(ckpt_path, map_location=device), strict=False)\n",
        "    val_loss, val_acc, y_true, y_pred = evaluate(model, vl)\n",
        "\n",
        "    # weighted precision/recall/f1 via sklearn\n",
        "    try:\n",
        "        report = classification_report(y_true, y_pred, digits=4, output_dict=True, zero_division=0)\n",
        "        wP = report[\"weighted avg\"][\"precision\"]\n",
        "        wR = report[\"weighted avg\"][\"recall\"]\n",
        "        wF = report[\"weighted avg\"][\"f1-score\"]\n",
        "        support = int(report[\"accuracy\"] * len(y_true))  # not right: fix below\n",
        "        support = int(len(y_true))                      # support = # val samples\n",
        "    except Exception:\n",
        "        wP = wR = wF = np.nan\n",
        "        support = len(y_true)\n",
        "\n",
        "    return {\n",
        "        \"StockCode\": code,\n",
        "        \"support\": support,\n",
        "        \"accuracy\": float(val_acc),\n",
        "        \"weighted_precision\": float(wP),\n",
        "        \"weighted_recall\": float(wR),\n",
        "        \"weighted_f1\": float(wF),\n",
        "        \"best_val_acc\": float(best_acc),\n",
        "        \"best_val_loss\": float(best_val),\n",
        "        \"ckpt_path\": (ckpt_path if save_ckpt else \"\"),\n",
        "        \"notes\": \"\"\n",
        "    }\n",
        "\n",
        "# ============================================\n",
        "# ğŸ“¥ Load parquet & get all codes\n",
        "# ============================================\n",
        "df_all = pd.read_parquet(PATH_PARQUET)\n",
        "for col in [CODE_COL, DATE_COL, LABEL_COL] + feature_cols:\n",
        "    if col not in df_all.columns:\n",
        "        raise ValueError(f\"Missing column: {col}\")\n",
        "df_all[LABEL_COL] = df_all[LABEL_COL].astype(\"int64\")\n",
        "\n",
        "codes = sorted(df_all[CODE_COL].dropna().astype(str).unique().tolist())\n",
        "print(f\"Found {len(codes)} unique StockCodes.\")\n",
        "\n",
        "# If you want to test on a subset first, uncomment:\n",
        "# codes = codes[:10]\n",
        "\n",
        "# ============================================\n",
        "# ğŸ” Loop all codes, collect metrics\n",
        "# ============================================\n",
        "all_rows = []\n",
        "pbar = tqdm(codes, desc=\"Per-code finetune\", ncols=100)\n",
        "for code in pbar:\n",
        "    pbar.set_postfix_str(code)\n",
        "    row = finetune_one_code(code, df_all, save_ckpt=True)\n",
        "    all_rows.append(row)\n",
        "\n",
        "metrics_df = pd.DataFrame(all_rows)\n",
        "metrics_csv = os.path.join(OUT_DIR, \"per_code_metrics.csv\")\n",
        "metrics_df.to_csv(metrics_csv, index=False)\n",
        "print(f\"\\nâœ… Saved per-code metrics to: {metrics_csv}\")\n",
        "\n",
        "# ============================================\n",
        "# ğŸ† Show Top-10 by requested metrics\n",
        "# ============================================\n",
        "def show_top10(df, col, higher_is_better=True):\n",
        "    df2 = df.copy()\n",
        "    df2 = df2.dropna(subset=[col])\n",
        "    df2 = df2.sort_values(col, ascending=not higher_is_better)\n",
        "    return df2[[\"StockCode\", col, \"support\", \"accuracy\", \"weighted_precision\", \"weighted_recall\", \"weighted_f1\"]].head(10)\n",
        "\n",
        "print(\"\\n=== Top-10 by Accuracy ===\")\n",
        "print(show_top10(metrics_df, \"accuracy\", True))\n",
        "\n",
        "print(\"\\n=== Top-10 by Support ===\")\n",
        "print(show_top10(metrics_df, \"support\", True))\n",
        "\n",
        "print(\"\\n=== Top-10 by Weighted Precision ===\")\n",
        "print(show_top10(metrics_df, \"weighted_precision\", True))\n",
        "\n",
        "print(\"\\n=== Top-10 by Weighted Recall ===\")\n",
        "print(show_top10(metrics_df, \"weighted_recall\", True))\n",
        "\n",
        "print(\"\\n=== Top-10 by Weighted F1 ===\")\n",
        "print(show_top10(metrics_df, \"weighted_f1\", True))\n",
        "\n",
        "print(f\"\\nğŸ“ Checkpoints & CSV are in: {OUT_DIR}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}