# =====================================
# ğŸš€ è‚¡ç¥¨æ–‡æœ¬ç‰¹å¾ç¼–ç ä¸»ç¨‹åºï¼ˆå¯¹é½ reference_keys_2024ï¼‰
# âœ… èŠ‚å‡æ—¥å½’å…¥ä¸Šä¸€ä¸ªäº¤æ˜“æ—¥ closing
# âœ… è¾“å‡º CSV + Parquet
# âœ… å›ºå®šè¾“å‡ºæ‰€æœ‰å‚è€ƒäº¤æ˜“æ—¥
# =====================================
import os
import glob
import json
import pandas as pd
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import torch.nn as nn

pool = None

# ---------- å·¥å…·å‡½æ•° ----------
def get_prev_trade_date(date, stock_ref_dates):
    """åœ¨è¯¥è‚¡ç¥¨äº¤æ˜“æ—¥åˆ—è¡¨ä¸­æ‰¾åˆ°ä¸Šä¸€ä¸ªäº¤æ˜“æ—¥"""
    date = pd.Timestamp(date)
    prev = [d for d in stock_ref_dates if pd.Timestamp(d) < date]
    return prev[-1] if prev else None


def map_time_period_vectorized(df, stock_ref_set, stock_ref_dates):
    """å‘é‡åŒ–æ—¶é—´æ®µæ˜ å°„ï¼ˆèŠ‚å‡æ—¥å½’å…¥ä¸Šä¸€ä¸ªäº¤æ˜“æ—¥ closingï¼‰"""
    df["hour"] = df["post_publish_time"].dt.hour
    df["minute"] = df["post_publish_time"].dt.minute
    df["date_only"] = df["post_publish_time"].dt.date
    df["minute_of_day"] = df["hour"] * 60 + df["minute"]

    df["time_period"] = np.where(
        ((df["minute_of_day"] >= 9 * 60 + 30) & (df["minute_of_day"] < 15 * 60)),
        "trading", "closing"
    )

    mapped_dates = []
    for d, t in zip(df["date_only"], df["minute_of_day"]):
        if d not in stock_ref_set:
            mapped_dates.append(get_prev_trade_date(d, stock_ref_dates))
        elif t < 9 * 60 + 30:  # æ—©ç›˜å‰å‘å¸– â†’ å‰ä¸€äº¤æ˜“æ—¥ closing
            mapped_dates.append(get_prev_trade_date(d, stock_ref_dates))
        else:
            mapped_dates.append(d)
    df["mapped_date"] = mapped_dates
    return df


'''
def encode_texts(texts, model, batch_size=1024):
    """å°†æ–‡æœ¬åˆ—è¡¨ç¼–ç ä¸ºå¹³å‡ embedding"""
    texts = [t.strip() for t in texts if isinstance(t, str) and t.strip()]
    if not texts:
        return np.zeros(model.get_sentence_embedding_dimension())
    embeddings = model.encode(
        texts,
        batch_size=batch_size,
        show_progress_bar=False,
        convert_to_numpy=True,
        normalize_embeddings=True,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    return embeddings.mean(axis=0)
'''
def encode_texts(texts, model, batch_size=1024):
    """
    å°†æ–‡æœ¬åˆ—è¡¨ç¼–ç ä¸ºå¹³å‡ embeddingã€‚
    å¦‚æœ 'pool' å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨å¤šè¿›ç¨‹ç¼–ç ã€‚
    """
    texts = [t.strip() for t in texts if isinstance(t, str) and t.strip()]
    if not texts:
        return np.zeros(model.get_sentence_embedding_dimension())

    # å†³å®šæ˜¯å¦è¿›è¡Œå½’ä¸€åŒ–
    # (å› ä¸º encode_multi_process ä¸æ”¯æŒï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨åš)
    do_normalization = True 

    global pool
    if pool:
        # --- å¤š GPU è·¯å¾„ ---
        embeddings = model.encode_multi_process(
            texts,
            pool=pool,
            batch_size=batch_size, # è¿™æ˜¯ *æ¯ä¸ª GPU* çš„ batch_size
        )
    else:
        # --- å• GPU / CPU è·¯å¾„ ---
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device) # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        embeddings = model.encode(
            texts,
            batch_size=batch_size, # è¿™æ˜¯æ€»çš„ batch_size
            show_progress_bar=False,
            convert_to_numpy=True,
            normalize_embeddings=do_normalization, # å• GPU æ”¯æŒè‡ªåŠ¨å½’ä¸€åŒ–
            device=device
        )
    
    # å¦‚æœä½¿ç”¨äº†å¤šè¿›ç¨‹ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å½’ä¸€åŒ–
    if pool and do_normalization:
        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
        # é¿å…é™¤ä»¥ 0
        norms[norms == 0] = 1e-12 
        embeddings = embeddings / norms
    elif pool and not do_normalization:
        # æ­¤æ—¶ embeddings å·²ç» ok
        pass
    elif not pool:
        # å• GPU è·¯å¾„å·²ç»å¤„ç†äº†å½’ä¸€åŒ– (normalize_embeddings=do_normalization)
        pass

    return embeddings.mean(axis=0)


# ---------- ä¸»å¤„ç†å‡½æ•° ----------
def process_text_encoder(file_path, ref_df, model, output_dir="processed_text_ref2024", save_parquet=True):
    stock_code = os.path.basename(file_path).split(".")[0].zfill(6)
    # print(f"\nğŸš€ æ­£åœ¨å¤„ç†è‚¡ç¥¨ {stock_code} ...")

    # å‚è€ƒäº¤æ˜“æ—¥è¡¨
    stock_ref_dates = sorted(ref_df[ref_df["stockbar_code"] == stock_code]["date"].tolist())
    if not stock_ref_dates:
        print(f"âš ï¸ è‚¡ç¥¨ {stock_code} æ— å‚è€ƒæ•°æ®ï¼Œè·³è¿‡ã€‚")
        return None
    stock_ref_set = set(stock_ref_dates)

    # è¯»å– parquet
    df = pd.read_parquet(file_path)
    df["post_publish_time"] = pd.to_datetime(df["post_publish_time"])

    # åº”ç”¨æ—¶é—´æ˜ å°„
    df = map_time_period_vectorized(df, stock_ref_set, stock_ref_dates)

    # æ£€æµ‹æ–‡æœ¬åˆ—
    TEXT_COL = next((c for c in ["text", "content", "post_content", "comment", "body"] if c in df.columns), None)
    if TEXT_COL is None:
        raise KeyError(f"âŒ æ‰¾ä¸åˆ°æ–‡æœ¬åˆ—ï¼š{df.columns.tolist()}")

    # åˆ†ç»„èšåˆ
    grouped = df.groupby(["stockbar_code", "mapped_date", "time_period"])[TEXT_COL].apply(list).reset_index()

    # ç¼–ç 
    embeddings = []
    for _, row in grouped.iterrows():
        emb = encode_texts(row[TEXT_COL], model)
        embeddings.append(emb)
    emb_array = np.vstack(embeddings)
    emb_cols = [f"dim_{i}" for i in range(emb_array.shape[1])]

    emb_df = pd.concat([
        grouped[["stockbar_code", "mapped_date", "time_period"]],
        pd.DataFrame(emb_array, columns=emb_cols)
    ], axis=1)

    # è½¬å®½è¡¨ (trading / closing)
    pivoted = (
        emb_df.pivot(index=["stockbar_code", "mapped_date"], columns="time_period", values=emb_cols)
        .reset_index()
    )
    pivoted.columns = ["_".join([c for c in col if c]) for col in pivoted.columns.values]

    # === æŒ‰å‚è€ƒæ—¥æœŸè¡¥é½ ===
    full_df = pd.DataFrame({
        "stockbar_code": stock_code,
        "mapped_date": stock_ref_dates
    })
    pivoted = pd.merge(full_df, pivoted, on=["stockbar_code", "mapped_date"], how="left")

    # ç¼ºå¤±å¡«å…… 0
    for col in pivoted.columns:
        if col not in ["stockbar_code", "mapped_date"]:
            pivoted[col] = pivoted[col].fillna(0.0)

    # ä¿å­˜
    csv_dir = os.path.join(output_dir, "csv")
    parquet_dir = os.path.join(output_dir, "parquet")
    os.makedirs(csv_dir, exist_ok=True)
    if save_parquet:
        os.makedirs(parquet_dir, exist_ok=True)

    out_csv = os.path.join(csv_dir, f"{stock_code}_text_features.csv")
    pivoted.round(6).to_csv(out_csv, index=False, encoding="utf-8-sig")

    if save_parquet:
        out_parquet = os.path.join(parquet_dir, f"{stock_code}_text_features.parquet")
        pivoted.to_parquet(out_parquet, index=False)

    # print(f"âœ… å®Œæˆ {stock_code} ({len(pivoted)} è¡Œ)")
    return stock_code


# ---------- æ‰¹é‡æ‰§è¡Œ ----------
def process_multiple_files(input_dir, ref_path, output_dir, save_parquet=True):
    files = sorted(glob.glob(os.path.join(input_dir, "*.parquet")))
    ref_df = pd.read_csv(ref_path)
    ref_df["stockbar_code"] = ref_df["stockbar_code"].astype(str).str.zfill(6)
    ref_df["date"] = pd.to_datetime(ref_df["date"]).dt.date

    os.makedirs(output_dir, exist_ok=True)
    log_file = os.path.join(output_dir, "processed_log.json")
    processed = set(json.load(open(log_file)) if os.path.exists(log_file) else [])

    print(f"ğŸ” æ£€æµ‹åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå·²å¤„ç† {len(processed)} ä¸ªã€‚")

    # åŠ è½½æ¨¡å‹ä¸€æ¬¡
    model_name = "BAAI/bge-large-zh-v1.5"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ§  åŠ è½½æ¨¡å‹: {model_name} ({device})")
    model = SentenceTransformer(model_name, device=device)

    if torch.cuda.is_available() and torch.cuda.device_count() > 1:
        print(f"ğŸš€ å¯åŠ¨å¤š GPU ç¼–ç ï¼Œå…± {torch.cuda.device_count()} ä¸ª GPUã€‚")

        # 2. å®šä¹‰ç›®æ ‡è®¾å¤‡
        target_devices = [f'cuda:{i}' for i in range(torch.cuda.device_count())]
    
        # 3. å¯åŠ¨å¤šè¿›ç¨‹æ± 
        # pool æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œkey æ˜¯è®¾å¤‡åï¼Œvalue æ˜¯è¿›ç¨‹
        global pool
        pool = model.start_multi_process_pool(target_devices=target_devices)

    for f in tqdm(files):
        stock_code = os.path.splitext(os.path.basename(f))[0].zfill(6)
        if stock_code in processed:
            continue
        try:
            result = process_text_encoder(f, ref_df, model, output_dir, save_parquet=save_parquet)
            if result:
                processed.add(result)
                json.dump(list(processed), open(log_file, "w", encoding="utf-8"),
                          ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ {stock_code}: {e}")

    print(f"\nğŸ‰ å®Œæˆï¼å…±å¤„ç† {len(processed)} åªè‚¡ç¥¨ã€‚")


# ---------- ä¸»å…¥å£ ----------
if __name__ == "__main__":
    root_path = '..'
    input_dir = f"{root_path}/csi300_senti_with_comments"                       # â† è¾“å…¥ç›®å½•
    ref_path = f"{root_path}/pack/reference_keys_2024.csv"  # â† å‚è€ƒäº¤æ˜“æ—¥
    output_dir = f"{root_path}/processed_text_ref2024"                # â† è¾“å‡ºç›®å½•
    process_multiple_files(input_dir, ref_path, output_dir, save_parquet=True)
